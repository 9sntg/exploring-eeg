{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2A: Image-Caption Retrieval with CLIP\n",
        "\n",
        "This notebook implements image-to-caption retrieval using CLIP (Contrastive Language-Image Pretraining).\n",
        "\n",
        "## Objectives:\n",
        "1. Load pretrained CLIP model from Hugging Face\n",
        "2. Compute embeddings for images and captions\n",
        "3. Perform image-to-caption retrieval using cosine similarity\n",
        "4. Evaluate retrieval performance with multiple metrics:\n",
        "   - Recall@K (instance-level and class-aware)\n",
        "   - BERTScore\n",
        "   - CLIPScore\n",
        "   - Mean Average Precision (MAP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install transformers torch torchvision pillow pandas numpy scikit-learn bert-score -q\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from bert_score import score as bert_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import average_precision_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load CLIP Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CLIP model and processor\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "print(f\"Loading CLIP model: {model_name}\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"CLIP model loaded successfully!\")\n",
        "print(f\"Image encoder: {model.vision_model.__class__.__name__}\")\n",
        "print(f\"Text encoder: {model.text_model.__class__.__name__}\")\n",
        "print(f\"Embedding dimension: {model.config.projection_dim}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading Functions\n",
        "\n",
        "We need to:\n",
        "1. Load images from the dataset\n",
        "2. Load captions from captions.txt\n",
        "3. Map images to their corresponding captions and categories\n",
        "4. Create train/val/test splits based on sessions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_captions(captions_path):\n",
        "    \"\"\"\n",
        "    Load captions from captions.txt file.\n",
        "    Expected format: image_name, caption, category\n",
        "    \"\"\"\n",
        "    captions_df = pd.read_csv(captions_path, sep='\\t', header=None, \n",
        "                              names=['image_name', 'caption', 'category'])\n",
        "    # Remove .jpg extension if present\n",
        "    captions_df['image_name'] = captions_df['image_name'].str.replace('.jpg', '').str.replace('.jpeg', '').str.replace('.png', '')\n",
        "    return captions_df\n",
        "\n",
        "def find_image_path(image_name, image_dirs):\n",
        "    \"\"\"\n",
        "    Find image file path given image name.\n",
        "    Searches in multiple possible directories and handles different extensions.\n",
        "    \"\"\"\n",
        "    extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n",
        "    \n",
        "    for img_dir in image_dirs:\n",
        "        for ext in extensions:\n",
        "            img_path = os.path.join(img_dir, f\"{image_name}{ext}\")\n",
        "            if os.path.exists(img_path):\n",
        "                return img_path\n",
        "            # Also try with extension already in name\n",
        "            img_path = os.path.join(img_dir, image_name)\n",
        "            if os.path.exists(img_path):\n",
        "                return img_path\n",
        "    \n",
        "    return None\n",
        "\n",
        "def load_image_caption_pairs(data_root, image_dirs, captions_path):\n",
        "    \"\"\"\n",
        "    Load all image-caption pairs from the dataset.\n",
        "    Returns a DataFrame with columns: image_name, image_path, caption, category\n",
        "    \"\"\"\n",
        "    # Load captions\n",
        "    captions_df = load_captions(captions_path)\n",
        "    \n",
        "    # Find image paths\n",
        "    image_paths = []\n",
        "    for img_name in tqdm(captions_df['image_name'], desc=\"Finding image paths\"):\n",
        "        img_path = find_image_path(img_name, image_dirs)\n",
        "        image_paths.append(img_path)\n",
        "    \n",
        "    captions_df['image_path'] = image_paths\n",
        "    \n",
        "    # Filter out images that weren't found\n",
        "    valid_mask = captions_df['image_path'].notna()\n",
        "    captions_df = captions_df[valid_mask].reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Loaded {len(captions_df)} valid image-caption pairs\")\n",
        "    print(f\"Categories: {captions_df['category'].nunique()}\")\n",
        "    \n",
        "    return captions_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Update these paths to match your dataset location\n",
        "DATA_ROOT = \"/path/to/your/dataset\"  # Root directory of your dataset\n",
        "IMAGE_DIRS = [\n",
        "    os.path.join(DATA_ROOT, \"images\"),\n",
        "    os.path.join(DATA_ROOT, \"All_images\"),\n",
        "    # Add other possible image directories\n",
        "]\n",
        "CAPTIONS_PATH = os.path.join(DATA_ROOT, \"captions.txt\")\n",
        "\n",
        "# Load image-caption pairs\n",
        "print(\"Loading dataset...\")\n",
        "dataset_df = load_image_caption_pairs(DATA_ROOT, IMAGE_DIRS, CAPTIONS_PATH)\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample data:\")\n",
        "print(dataset_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Embeddings\n",
        "\n",
        "We'll compute embeddings for all images and captions in batches for efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_image_embeddings(model, processor, image_paths, batch_size=32, device='cuda'):\n",
        "    \"\"\"\n",
        "    Compute CLIP embeddings for a list of images.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Computing image embeddings\"):\n",
        "            batch_paths = image_paths[i:i+batch_size]\n",
        "            \n",
        "            # Load and preprocess images\n",
        "            images = []\n",
        "            valid_indices = []\n",
        "            for j, img_path in enumerate(batch_paths):\n",
        "                try:\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    images.append(img)\n",
        "                    valid_indices.append(i + j)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {img_path}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if not images:\n",
        "                continue\n",
        "            \n",
        "            # Process images\n",
        "            inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "            \n",
        "            # Get image embeddings\n",
        "            image_features = model.get_image_features(**inputs)\n",
        "            \n",
        "            # Normalize embeddings\n",
        "            image_features = F.normalize(image_features, p=2, dim=1)\n",
        "            \n",
        "            # Store embeddings (handle missing images)\n",
        "            batch_embeddings = torch.zeros(len(batch_paths), image_features.shape[1], device=device)\n",
        "            for idx, valid_idx in enumerate(valid_indices):\n",
        "                batch_embeddings[valid_idx - i] = image_features[idx]\n",
        "            \n",
        "            embeddings.append(batch_embeddings.cpu())\n",
        "    \n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "def compute_text_embeddings(model, processor, texts, batch_size=32, device='cuda'):\n",
        "    \"\"\"\n",
        "    Compute CLIP embeddings for a list of text captions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing text embeddings\"):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            \n",
        "            # Process texts\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            \n",
        "            # Get text embeddings\n",
        "            text_features = model.get_text_features(**inputs)\n",
        "            \n",
        "            # Normalize embeddings\n",
        "            text_features = F.normalize(text_features, p=2, dim=1)\n",
        "            \n",
        "            embeddings.append(text_features.cpu())\n",
        "    \n",
        "    return torch.cat(embeddings, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute embeddings for all images and captions\n",
        "print(\"Computing image embeddings...\")\n",
        "image_embeddings = compute_image_embeddings(\n",
        "    model, processor, \n",
        "    dataset_df['image_path'].tolist(), \n",
        "    batch_size=32, \n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Computing text embeddings...\")\n",
        "text_embeddings = compute_text_embeddings(\n",
        "    model, processor,\n",
        "    dataset_df['caption'].tolist(),\n",
        "    batch_size=32,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nImage embeddings shape: {image_embeddings.shape}\")\n",
        "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Image-to-Caption Retrieval\n",
        "\n",
        "Implement retrieval using cosine similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_captions(image_embedding, caption_embeddings, k=5):\n",
        "    \"\"\"\n",
        "    Retrieve top-K captions for a given image embedding.\n",
        "    \n",
        "    Args:\n",
        "        image_embedding: Single image embedding (1, d)\n",
        "        caption_embeddings: All caption embeddings (N, d)\n",
        "        k: Number of top captions to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        top_k_indices: Indices of top-K captions\n",
        "        top_k_scores: Similarity scores for top-K captions\n",
        "    \"\"\"\n",
        "    # Compute cosine similarity (dot product since embeddings are normalized)\n",
        "    similarities = torch.matmul(image_embedding, caption_embeddings.T)  # (1, N)\n",
        "    \n",
        "    # Get top-K indices\n",
        "    top_k_scores, top_k_indices = torch.topk(similarities, k=k, dim=1)\n",
        "    \n",
        "    return top_k_indices.squeeze(0).cpu().numpy(), top_k_scores.squeeze(0).cpu().numpy()\n",
        "\n",
        "def batch_retrieve_captions(image_embeddings, caption_embeddings, k=5):\n",
        "    \"\"\"\n",
        "    Retrieve top-K captions for all images.\n",
        "    \n",
        "    Returns:\n",
        "        all_top_k_indices: (N_images, k) array of caption indices\n",
        "        all_top_k_scores: (N_images, k) array of similarity scores\n",
        "    \"\"\"\n",
        "    # Move to device if needed\n",
        "    if image_embeddings.device != caption_embeddings.device:\n",
        "        image_embeddings = image_embeddings.to(caption_embeddings.device)\n",
        "    \n",
        "    # Compute similarity matrix\n",
        "    similarity_matrix = torch.matmul(image_embeddings, caption_embeddings.T)  # (N_images, N_captions)\n",
        "    \n",
        "    # Get top-K for each image\n",
        "    top_k_scores, top_k_indices = torch.topk(similarity_matrix, k=k, dim=1)\n",
        "    \n",
        "    return top_k_indices.cpu().numpy(), top_k_scores.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform retrieval for all images\n",
        "print(\"Performing retrieval...\")\n",
        "image_embeddings_gpu = image_embeddings.to(device)\n",
        "text_embeddings_gpu = text_embeddings.to(device)\n",
        "\n",
        "# Get top-5 retrievals\n",
        "top_k = 5\n",
        "retrieved_indices, retrieved_scores = batch_retrieve_captions(\n",
        "    image_embeddings_gpu, text_embeddings_gpu, k=top_k\n",
        ")\n",
        "\n",
        "print(f\"Retrieved top-{top_k} captions for {len(retrieved_indices)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation Metrics\n",
        "\n",
        "Implement all required evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_recall_at_k(retrieved_indices, ground_truth_indices, k_values=[1, 3, 5], instance_level=True):\n",
        "    \"\"\"\n",
        "    Compute Recall@K metrics.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_indices: (N_images, max_k) array of retrieved caption indices\n",
        "        ground_truth_indices: (N_images,) array of ground truth caption indices\n",
        "        k_values: List of K values to compute\n",
        "        instance_level: If True, exact match required. If False, class-aware.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of Recall@K scores\n",
        "    \"\"\"\n",
        "    recalls = {}\n",
        "    \n",
        "    for k in k_values:\n",
        "        if k > retrieved_indices.shape[1]:\n",
        "            continue\n",
        "        \n",
        "        correct = 0\n",
        "        for i, gt_idx in enumerate(ground_truth_indices):\n",
        "            top_k_retrieved = retrieved_indices[i, :k]\n",
        "            if instance_level:\n",
        "                # Instance-level: exact caption match\n",
        "                if gt_idx in top_k_retrieved:\n",
        "                    correct += 1\n",
        "            else:\n",
        "                # Class-aware: any caption from same category\n",
        "                # This requires category information - will be implemented below\n",
        "                if gt_idx in top_k_retrieved:\n",
        "                    correct += 1\n",
        "        \n",
        "        recalls[f'Recall@{k}'] = correct / len(ground_truth_indices)\n",
        "    \n",
        "    return recalls\n",
        "\n",
        "def compute_recall_at_k_class_aware(retrieved_indices, ground_truth_indices, categories, k_values=[1, 3, 5]):\n",
        "    \"\"\"\n",
        "    Compute class-aware Recall@K (any caption from same category counts as correct).\n",
        "    \"\"\"\n",
        "    recalls = {}\n",
        "    \n",
        "    # Create category mapping\n",
        "    category_map = {}\n",
        "    for idx, cat in enumerate(categories):\n",
        "        if cat not in category_map:\n",
        "            category_map[cat] = []\n",
        "        category_map[cat].append(idx)\n",
        "    \n",
        "    for k in k_values:\n",
        "        if k > retrieved_indices.shape[1]:\n",
        "            continue\n",
        "        \n",
        "        correct = 0\n",
        "        for i, gt_idx in enumerate(ground_truth_indices):\n",
        "            gt_category = categories[gt_idx]\n",
        "            top_k_retrieved = retrieved_indices[i, :k]\n",
        "            \n",
        "            # Check if any retrieved caption is from the same category\n",
        "            found = False\n",
        "            for ret_idx in top_k_retrieved:\n",
        "                if categories[ret_idx] == gt_category:\n",
        "                    found = True\n",
        "                    break\n",
        "            \n",
        "            if found:\n",
        "                correct += 1\n",
        "        \n",
        "        recalls[f'Class-aware Recall@{k}'] = correct / len(ground_truth_indices)\n",
        "    \n",
        "    return recalls\n",
        "\n",
        "def compute_map_caption_level(similarity_matrix, ground_truth_indices):\n",
        "    \"\"\"\n",
        "    Compute Mean Average Precision (MAP) at caption level.\n",
        "    Only exact ground-truth caption is considered relevant.\n",
        "    \"\"\"\n",
        "    aps = []\n",
        "    \n",
        "    for i, gt_idx in enumerate(ground_truth_indices):\n",
        "        # Get similarity scores for this image\n",
        "        scores = similarity_matrix[i].cpu().numpy()\n",
        "        \n",
        "        # Create binary relevance labels (only ground truth is relevant)\n",
        "        y_true = np.zeros(len(scores))\n",
        "        y_true[gt_idx] = 1\n",
        "        \n",
        "        # Compute average precision\n",
        "        ap = average_precision_score(y_true, scores)\n",
        "        aps.append(ap)\n",
        "    \n",
        "    return np.mean(aps)\n",
        "\n",
        "def compute_map_class_aware(similarity_matrix, ground_truth_indices, categories):\n",
        "    \"\"\"\n",
        "    Compute class-aware MAP (any caption from same category is relevant).\n",
        "    \"\"\"\n",
        "    aps = []\n",
        "    \n",
        "    # Create category mapping\n",
        "    category_map = {}\n",
        "    for idx, cat in enumerate(categories):\n",
        "        if cat not in category_map:\n",
        "            category_map[cat] = []\n",
        "        category_map[cat].append(idx)\n",
        "    \n",
        "    for i, gt_idx in enumerate(ground_truth_indices):\n",
        "        gt_category = categories[gt_idx]\n",
        "        \n",
        "        # Get similarity scores for this image\n",
        "        scores = similarity_matrix[i].cpu().numpy()\n",
        "        \n",
        "        # Create binary relevance labels (all captions from same category are relevant)\n",
        "        y_true = np.zeros(len(scores))\n",
        "        for relevant_idx in category_map[gt_category]:\n",
        "            y_true[relevant_idx] = 1\n",
        "        \n",
        "        # Compute average precision\n",
        "        ap = average_precision_score(y_true, scores)\n",
        "        aps.append(ap)\n",
        "    \n",
        "    return np.mean(aps)\n",
        "\n",
        "def compute_map_per_class(similarity_matrix, ground_truth_indices, categories):\n",
        "    \"\"\"\n",
        "    Compute MAP separately for each semantic class.\n",
        "    \"\"\"\n",
        "    unique_categories = sorted(set(categories))\n",
        "    map_per_class = {}\n",
        "    \n",
        "    for cat in unique_categories:\n",
        "        # Get indices for this category\n",
        "        cat_indices = [i for i, c in enumerate(categories) if c == cat]\n",
        "        \n",
        "        if not cat_indices:\n",
        "            continue\n",
        "        \n",
        "        # Compute MAP for this category\n",
        "        cat_aps = []\n",
        "        for i, gt_idx in enumerate(ground_truth_indices):\n",
        "            if categories[gt_idx] != cat:\n",
        "                continue\n",
        "            \n",
        "            scores = similarity_matrix[i].cpu().numpy()\n",
        "            y_true = np.zeros(len(scores))\n",
        "            y_true[gt_idx] = 1\n",
        "            \n",
        "            ap = average_precision_score(y_true, scores)\n",
        "            cat_aps.append(ap)\n",
        "        \n",
        "        map_per_class[cat] = np.mean(cat_aps) if cat_aps else 0.0\n",
        "    \n",
        "    return map_per_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ground truth indices (each image's true caption index)\n",
        "ground_truth_indices = np.arange(len(dataset_df))  # Assuming 1-to-1 mapping\n",
        "\n",
        "# Compute similarity matrix for MAP calculation\n",
        "similarity_matrix = torch.matmul(image_embeddings_gpu, text_embeddings_gpu.T)\n",
        "\n",
        "# Compute Recall@K metrics\n",
        "print(\"Computing Recall@K metrics...\")\n",
        "recall_instance = compute_recall_at_k(\n",
        "    retrieved_indices, ground_truth_indices, k_values=[1, 3, 5], instance_level=True\n",
        ")\n",
        "recall_class_aware = compute_recall_at_k_class_aware(\n",
        "    retrieved_indices, ground_truth_indices, \n",
        "    dataset_df['category'].values, k_values=[1, 3, 5]\n",
        ")\n",
        "\n",
        "print(\"\\n=== Recall Metrics ===\")\n",
        "for k, score in recall_instance.items():\n",
        "    print(f\"{k}: {score:.4f}\")\n",
        "for k, score in recall_class_aware.items():\n",
        "    print(f\"{k}: {score:.4f}\")\n",
        "\n",
        "# Compute MAP metrics\n",
        "print(\"\\nComputing MAP metrics...\")\n",
        "map_caption = compute_map_caption_level(similarity_matrix, ground_truth_indices)\n",
        "map_class_aware = compute_map_class_aware(\n",
        "    similarity_matrix, ground_truth_indices, dataset_df['category'].values\n",
        ")\n",
        "map_per_class = compute_map_per_class(\n",
        "    similarity_matrix, ground_truth_indices, dataset_df['category'].values\n",
        ")\n",
        "\n",
        "print(\"\\n=== MAP Metrics ===\")\n",
        "print(f\"Caption-level MAP: {map_caption:.4f}\")\n",
        "print(f\"Class-aware MAP: {map_class_aware:.4f}\")\n",
        "print(\"\\nPer-class MAP:\")\n",
        "for cat, score in sorted(map_per_class.items()):\n",
        "    print(f\"  {cat}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Semantic Metrics: BERTScore and CLIPScore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_clipscore(image_embeddings, text_embeddings, retrieved_indices, ground_truth_indices):\n",
        "    \"\"\"\n",
        "    Compute CLIPScore for retrieved captions.\n",
        "    CLIPScore = cosine similarity between image and text embeddings.\n",
        "    \"\"\"\n",
        "    clip_scores = []\n",
        "    \n",
        "    for i, gt_idx in enumerate(ground_truth_indices):\n",
        "        # Get top-1 retrieved caption\n",
        "        ret_idx = retrieved_indices[i, 0]\n",
        "        \n",
        "        # Compute similarity between image and retrieved caption\n",
        "        img_emb = image_embeddings[i:i+1]\n",
        "        txt_emb = text_embeddings[ret_idx:ret_idx+1]\n",
        "        similarity = torch.matmul(img_emb, txt_emb.T).item()\n",
        "        \n",
        "        clip_scores.append(similarity)\n",
        "    \n",
        "    return np.array(clip_scores)\n",
        "\n",
        "def compute_bertscore_batch(candidates, references, batch_size=32):\n",
        "    \"\"\"\n",
        "    Compute BERTScore for candidate and reference captions.\n",
        "    \"\"\"\n",
        "    all_scores = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(candidates), batch_size), desc=\"Computing BERTScore\"):\n",
        "        batch_candidates = candidates[i:i+batch_size]\n",
        "        batch_references = references[i:i+batch_size]\n",
        "        \n",
        "        P, R, F1 = bert_score(\n",
        "            batch_candidates, batch_references,\n",
        "            lang='en', verbose=False, device=device\n",
        "        )\n",
        "        \n",
        "        all_scores.append(F1.cpu().numpy())\n",
        "    \n",
        "    return np.concatenate(all_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute CLIPScore\n",
        "print(\"Computing CLIPScore...\")\n",
        "clip_scores = compute_clipscore(\n",
        "    image_embeddings_gpu, text_embeddings_gpu,\n",
        "    retrieved_indices, ground_truth_indices\n",
        ")\n",
        "\n",
        "print(f\"CLIPScore - Mean: {clip_scores.mean():.4f}, Std: {clip_scores.std():.4f}\")\n",
        "print(f\"CLIPScore - Min: {clip_scores.min():.4f}, Max: {clip_scores.max():.4f}\")\n",
        "\n",
        "# Compute BERTScore for top-1 retrievals\n",
        "print(\"\\nComputing BERTScore...\")\n",
        "retrieved_captions = [dataset_df.iloc[retrieved_indices[i, 0]]['caption'] for i in range(len(ground_truth_indices))]\n",
        "ground_truth_captions = dataset_df['caption'].tolist()\n",
        "\n",
        "# Sample a subset for BERTScore (it can be slow for large datasets)\n",
        "sample_size = min(1000, len(ground_truth_captions))\n",
        "sample_indices = np.random.choice(len(ground_truth_captions), sample_size, replace=False)\n",
        "\n",
        "sample_retrieved = [retrieved_captions[i] for i in sample_indices]\n",
        "sample_ground_truth = [ground_truth_captions[i] for i in sample_indices]\n",
        "\n",
        "bert_scores = compute_bertscore_batch(sample_retrieved, sample_ground_truth)\n",
        "\n",
        "print(f\"\\nBERTScore F1 - Mean: {bert_scores.mean():.4f}, Std: {bert_scores.std():.4f}\")\n",
        "print(f\"High similarity rate (F1 > 0.7): {(bert_scores > 0.7).mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare CLIPScore for matched vs mismatched pairs\n",
        "print(\"Analyzing CLIPScore distribution...\")\n",
        "\n",
        "# Matched pairs (ground truth)\n",
        "matched_scores = []\n",
        "for i, gt_idx in enumerate(ground_truth_indices):\n",
        "    img_emb = image_embeddings_gpu[i:i+1]\n",
        "    txt_emb = text_embeddings_gpu[gt_idx:gt_idx+1]\n",
        "    similarity = torch.matmul(img_emb, txt_emb.T).item()\n",
        "    matched_scores.append(similarity)\n",
        "\n",
        "# Mismatched pairs (random negative samples)\n",
        "mismatched_scores = []\n",
        "for i in range(min(1000, len(ground_truth_indices))):\n",
        "    img_emb = image_embeddings_gpu[i:i+1]\n",
        "    # Random negative caption\n",
        "    neg_idx = np.random.choice([j for j in range(len(text_embeddings_gpu)) if j != ground_truth_indices[i]])\n",
        "    txt_emb = text_embeddings_gpu[neg_idx:neg_idx+1]\n",
        "    similarity = torch.matmul(img_emb, txt_emb.T).item()\n",
        "    mismatched_scores.append(similarity)\n",
        "\n",
        "matched_scores = np.array(matched_scores)\n",
        "mismatched_scores = np.array(mismatched_scores)\n",
        "\n",
        "print(f\"\\nMatched pairs - Mean: {matched_scores.mean():.4f}, Std: {matched_scores.std():.4f}\")\n",
        "print(f\"Mismatched pairs - Mean: {mismatched_scores.mean():.4f}, Std: {mismatched_scores.std():.4f}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(matched_scores, bins=50, alpha=0.7, label='Matched pairs', density=True)\n",
        "plt.hist(mismatched_scores, bins=50, alpha=0.7, label='Mismatched pairs', density=True)\n",
        "plt.xlabel('CLIPScore (Cosine Similarity)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('CLIPScore Distribution: Matched vs Mismatched Image-Caption Pairs')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some retrieval examples\n",
        "def visualize_retrieval_examples(dataset_df, retrieved_indices, ground_truth_indices, \n",
        "                                 image_embeddings_gpu, text_embeddings_gpu, \n",
        "                                 num_examples=5, top_k=3):\n",
        "    \"\"\"\n",
        "    Visualize retrieval examples with images and captions.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(num_examples, top_k + 1, figsize=(15, 3*num_examples))\n",
        "    \n",
        "    for i in range(num_examples):\n",
        "        idx = i * (len(ground_truth_indices) // num_examples)\n",
        "        gt_idx = ground_truth_indices[idx]\n",
        "        \n",
        "        # Load and display query image\n",
        "        try:\n",
        "            img = Image.open(dataset_df.iloc[idx]['image_path'])\n",
        "            axes[i, 0].imshow(img)\n",
        "            axes[i, 0].set_title(f\"Query Image\\nGT: {dataset_df.iloc[gt_idx]['caption'][:50]}...\")\n",
        "            axes[i, 0].axis('off')\n",
        "        except:\n",
        "            axes[i, 0].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
        "            axes[i, 0].axis('off')\n",
        "        \n",
        "        # Display retrieved captions\n",
        "        for j in range(top_k):\n",
        "            ret_idx = retrieved_indices[idx, j]\n",
        "            ret_caption = dataset_df.iloc[ret_idx]['caption']\n",
        "            similarity = torch.matmul(\n",
        "                image_embeddings_gpu[idx:idx+1],\n",
        "                text_embeddings_gpu[ret_idx:ret_idx+1].T\n",
        "            ).item()\n",
        "            \n",
        "            # Color: green if correct, red if wrong\n",
        "            color = 'green' if ret_idx == gt_idx else 'red'\n",
        "            axes[i, j+1].text(0.1, 0.5, f\"Rank {j+1}\\n{ret_caption[:80]}...\\nSim: {similarity:.3f}\",\n",
        "                           ha='left', va='center', fontsize=9, \n",
        "                           bbox=dict(boxstyle='round', facecolor=color, alpha=0.3))\n",
        "            axes[i, j+1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Uncomment to visualize (requires images to be loaded)\n",
        "# visualize_retrieval_examples(dataset_df, retrieved_indices, ground_truth_indices,\n",
        "#                              image_embeddings_gpu, text_embeddings_gpu, num_examples=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary Report\n",
        "\n",
        "Compile all results into a summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile summary report\n",
        "print(\"=\" * 60)\n",
        "print(\"TASK 2A: IMAGE-CAPTION RETRIEVAL WITH CLIP - RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. RETRIEVAL METRICS (Recall@K)\")\n",
        "print(\"-\" * 60)\n",
        "for k, score in recall_instance.items():\n",
        "    print(f\"  {k:25s}: {score:.4f}\")\n",
        "for k, score in recall_class_aware.items():\n",
        "    print(f\"  {k:25s}: {score:.4f}\")\n",
        "\n",
        "print(\"\\n2. MEAN AVERAGE PRECISION (MAP)\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"  Caption-level MAP:        {map_caption:.4f}\")\n",
        "print(f\"  Class-aware MAP:           {map_class_aware:.4f}\")\n",
        "\n",
        "print(\"\\n3. SEMANTIC METRICS\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"  CLIPScore (mean):          {clip_scores.mean():.4f} ± {clip_scores.std():.4f}\")\n",
        "print(f\"  CLIPScore (matched):       {matched_scores.mean():.4f} ± {matched_scores.std():.4f}\")\n",
        "print(f\"  CLIPScore (mismatched):     {mismatched_scores.mean():.4f} ± {mismatched_scores.std():.4f}\")\n",
        "print(f\"  BERTScore F1 (mean):       {bert_scores.mean():.4f} ± {bert_scores.std():.4f}\")\n",
        "print(f\"  High similarity rate (>0.7): {(bert_scores > 0.7).mean():.4f}\")\n",
        "\n",
        "print(\"\\n4. PER-CLASS MAP\")\n",
        "print(\"-\" * 60)\n",
        "for cat, score in sorted(map_per_class.items()):\n",
        "    print(f\"  {str(cat):20s}: {score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Analysis:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"• Instance-level Recall@1: {recall_instance['Recall@1']:.2%} - CLIP can retrieve\")\n",
        "print(f\"  the exact ground-truth caption for {recall_instance['Recall@1']:.2%} of images.\")\n",
        "print(f\"• Class-aware Recall@5: {recall_class_aware['Class-aware Recall@5']:.2%} - CLIP can\")\n",
        "print(f\"  retrieve a caption from the same category for {recall_class_aware['Class-aware Recall@5']:.2%} of images.\")\n",
        "print(f\"• CLIPScore separation: {matched_scores.mean() - mismatched_scores.mean():.4f} difference\")\n",
        "print(f\"  between matched and mismatched pairs indicates good alignment.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
