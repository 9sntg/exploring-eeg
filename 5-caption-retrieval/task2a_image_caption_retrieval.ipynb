{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2A: Image-Caption Retrieval with CLIP\n",
        "\n",
        "This notebook implements image-to-caption retrieval using CLIP (Contrastive Language-Image Pretraining).\n",
        "\n",
        "## Objectives:\n",
        "1. Load pretrained CLIP model from Hugging Face\n",
        "2. Compute embeddings for images and captions\n",
        "3. Perform image-to-caption retrieval using cosine similarity\n",
        "4. Evaluate retrieval performance with multiple metrics:\n",
        "   - Recall@K (instance-level and class-aware)\n",
        "   - BERTScore\n",
        "   - CLIPScore\n",
        "   - Mean Average Precision (MAP)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
