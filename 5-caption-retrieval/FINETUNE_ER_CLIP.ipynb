{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project: Task 2A: Image-Caption Retrieval with CLIP\n",
    "\n",
    "This notebook implements image-to-caption retrieval using CLIP (Contrastive Language-Image Pretraining).\n",
    "\n",
    "## Names\n",
    "1. Diego Bermudez Sierra - dabermud@andrew.cmu.edu\n",
    "2. Santiago Bolaños Vega - sbolaosv@andrew.cmu.edu\n",
    "\n",
    "## Objectives:\n",
    "1. Load pretrained CLIP model from Hugging Face\n",
    "2. Compute embeddings for images and captions\n",
    "3. Perform image-to-caption retrieval using cosine similarity\n",
    "4. Evaluate retrieval performance with multiple metrics:\n",
    "   - Recall@K (instance-level and class-aware)\n",
    "   - BERTScore\n",
    "   - CLIPScore\n",
    "   - Mean Average Precision (MAP)\n",
    "\n",
    "## AI Acknowledgement\n",
    "This task was completed with assistance from an AI coding assistant (Auto, powered by Cursor) that helped implement and debug the code for Task 2A: Image-Caption Retrieval with CLIP. The assistant provided guidance on environment setup, resolved PyTorch CUDA compatibility issues, fixed CLIP model loading with safetensors support, corrected data loading functions to properly parse the captions.txt file format, and helped configure the GPU environment on Bridges2. All code implementations, debugging, and final solutions were developed collaboratively through iterative problem-solving sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'root_eeg': \"/jet/home/bermudez/exploring-eeg/4-eeg-classifiers\",\n",
    "    'root_clip': \"/jet/home/bermudez/exploring-eeg/5-caption-retrieval\",\n",
    "    'project_root': \"/jet/home/bermudez/exploring-eeg\",\n",
    "    'eeg_results_path': \"/results/Finetune_Multihead\",\n",
    "    'data_root': \"/ocean/projects/cis250019p/gandotra/11785-gp-eeg\",\n",
    "    'model_name': \"openai/clip-vit-base-patch32\",\n",
    "    'epochs': 10,\n",
    "    'batch_size': 128,\n",
    "    'lora_rank': 32,\n",
    "    'temperature': 0.07,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "EEG results dir: /jet/home/bermudez/exploring-eeg/4-eeg-classifiers/results/Finetune_Multihead\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(config['root_eeg'])\n",
    "\n",
    "# Path to this folder (CLIP utilities live in utils_clip/)\n",
    "sys.path.append(config['root_clip'])\n",
    "\n",
    "\n",
    "from data_extraction.dataset_builder import build_trial_index, split_by_session\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "PROJECT_ROOT = Path(config['project_root'])\n",
    "\n",
    "EEG_DIR      = Path(config['root_eeg'])\n",
    "CAPTION_DIR  = Path(config['root_clip'])\n",
    "\n",
    "DATA_ROOT = Path(config['data_root'])\n",
    "CAPTIONS_PATH = DATA_ROOT / \"captions.txt\"\n",
    "\n",
    "EEG_RESULTS_DIR = Path(config['root_eeg']+config['eeg_results_path'])\n",
    "print(\"EEG results dir:\", EEG_RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model: openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embedding dim: 512\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "model_name = config['model_name']\n",
    "print(f\"Loading CLIP model: {model_name}\")\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(model_name, use_safetensors=True).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "clip_model.eval()\n",
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False  # we'll only train our own adapter + EEG head\n",
    "\n",
    "text_embed_dim = clip_model.config.projection_dim\n",
    "print(\"Text embedding dim:\", text_embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load EEG embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: features (15600, 256), meta (15600, 6)\n",
      "val: features (5200, 256), meta (5200, 6)\n",
      "test: features (5200, 256), meta (5200, 6)\n",
      "EEG feature dim: 256\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_eeg_split(split: str, base_dir: Path):\n",
    "    \"\"\"\n",
    "    Load EEG features and metadata for a given split: 'train', 'val', 'test'.\n",
    "    \"\"\"\n",
    "    feats_path = base_dir / f\"multihead_{split}_embeddings.npy\"\n",
    "    meta_path  = base_dir / f\"multihead_{split}_meta.csv\"\n",
    "\n",
    "    assert feats_path.exists(), f\"Missing {feats_path}\"\n",
    "    assert meta_path.exists(),  f\"Missing {meta_path}\"\n",
    "\n",
    "    feats = np.load(feats_path)          # [N, hidden_dim]\n",
    "    meta  = pd.read_csv(meta_path)       # columns: subject, session, run, image_name, class_id\n",
    "\n",
    "    print(f\"{split}: features {feats.shape}, meta {meta.shape}\")\n",
    "    return feats, meta\n",
    "\n",
    "eeg_train_feats, eeg_train_meta = load_eeg_split(\"train\", EEG_RESULTS_DIR)\n",
    "eeg_val_feats,   eeg_val_meta   = load_eeg_split(\"val\",   EEG_RESULTS_DIR)\n",
    "eeg_test_feats,  eeg_test_meta  = load_eeg_split(\"test\",  EEG_RESULTS_DIR)\n",
    "\n",
    "eeg_feat_dim = eeg_train_feats.shape[1]\n",
    "print(\"EEG feature dim:\", eeg_feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 519 EEG run files under /ocean/projects/cis250019p/gandotra/11785-gp-eeg\n",
      "Indexed 26000 EEG trials from 519 runs.\n",
      "Loaded 9825 caption entries from captions.txt\n",
      "Merged 9825 labels from captions.txt.\n",
      "Filtered labeled EEG trials: 26000/26000 remain (100.0%)\n",
      "Split summary:\n",
      "split\n",
      "train    15600\n",
      "test      5200\n",
      "val       5200\n",
      "Name: count, dtype: int64\n",
      "Index df columns: ['subject', 'session', 'run', 'trial', 'eeg_path', 'csv_path', 'image_name', 'image_key', 'category', 'class_id', 'abstracted', 'split']\n",
      "Index df head:\n",
      "  subject session     run  trial  \\\n",
      "0  sub-02  ses-01  run-01      0   \n",
      "1  sub-02  ses-01  run-01      1   \n",
      "2  sub-02  ses-01  run-01      2   \n",
      "3  sub-02  ses-01  run-01      3   \n",
      "4  sub-02  ses-01  run-01      4   \n",
      "\n",
      "                                            eeg_path  \\\n",
      "0  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "1  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "2  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "3  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "4  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "\n",
      "                                            csv_path  \\\n",
      "0  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "1  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "2  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "3  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "4  /ocean/projects/cis250019p/gandotra/11785-gp-e...   \n",
      "\n",
      "                                          image_name        image_key  \\\n",
      "0  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...  n03201208_14446   \n",
      "1  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...   n02410509_3128   \n",
      "2  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...      2010_005066   \n",
      "3  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...      2009_000920   \n",
      "4  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...   n04344873_6021   \n",
      "\n",
      "      category  class_id                                         abstracted  \\\n",
      "0  diningtable        10  Wooden diningtable with matching chairs on har...   \n",
      "1          cow         9             Cow grazing on green pasture near barn   \n",
      "2        sheep        16            Sheep standing on hay inside yellow pen   \n",
      "3        train        18       Red train on tracks near apartment buildings   \n",
      "4         sofa        17             Red patterned sofa on a concrete floor   \n",
      "\n",
      "  split  \n",
      "0  test  \n",
      "1  test  \n",
      "2  test  \n",
      "3  test  \n",
      "4  test  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "index_df = build_trial_index(DATA_ROOT, CAPTIONS_PATH)\n",
    "index_df = split_by_session(index_df)\n",
    "\n",
    "print(\"Index df columns:\", index_df.columns.tolist())\n",
    "print(\"Index df head:\")\n",
    "print(index_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condensed index shape: (26000, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train meta with captions:   subject session     run                                         image_name  \\\n",
      "0  sub-23  ses-02  run-02  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...   \n",
      "1  sub-19  ses-01  run-03  C:\\Users\\casia\\Desktop\\eeg_code_exp-info\\pic_1...   \n",
      "2  sub-14  ses-01  run-03  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...   \n",
      "3  sub-15  ses-01  run-04  C:\\Users\\Huawei\\Desktop\\eeg_pascal_imagenet\\pi...   \n",
      "4  sub-02  ses-04  run-01  C:\\Users\\casia\\Desktop\\eeg_code_exp-info\\pic_1...   \n",
      "\n",
      "  category_x  class_id category_y  \\\n",
      "0       boat         3       boat   \n",
      "1      horse        13      horse   \n",
      "2      train        18      train   \n",
      "3       sofa        17       sofa   \n",
      "4      train        18      train   \n",
      "\n",
      "                                          abstracted  \n",
      "0       Boat carrying containers on calm ocean water  \n",
      "1              Young horse walking on grassy pasture  \n",
      "2        Freight train traveling through a rail yard  \n",
      "3  Sofa with pillows near a decorated Christmas tree  \n",
      "4       Green train on railway tracks near buildings  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Select only the fields we need from index_df to avoid duplication\n",
    "index_key_cols = [\n",
    "    \"subject\", \"session\", \"run\", \"image_name\",\n",
    "    \"category\", \"class_id\", \"abstracted\"\n",
    "]\n",
    "\n",
    "index_small = index_df[index_key_cols].drop_duplicates()\n",
    "print(\"Condensed index shape:\", index_small.shape)\n",
    "\n",
    "def attach_captions(eeg_meta: pd.DataFrame, index_small: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge EEG meta with index_small to add 'category' and 'abstracted' (caption).\n",
    "    \"\"\"\n",
    "    merged = eeg_meta.merge(\n",
    "        index_small,\n",
    "        on=[\"subject\", \"session\", \"run\", \"image_name\", \"class_id\"],\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "    missing = merged[\"abstracted\"].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"Warning: {missing} trials could not be matched to captions.\")\n",
    "    return merged\n",
    "\n",
    "eeg_train_meta_full = attach_captions(eeg_train_meta, index_small)\n",
    "eeg_val_meta_full   = attach_captions(eeg_val_meta, index_small)\n",
    "eeg_test_meta_full  = attach_captions(eeg_test_meta, index_small)\n",
    "\n",
    "print(\"Train meta with captions:\", eeg_train_meta_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes — train: 15600 val: 5200 test: 5200\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "class EEGCaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset: EEG feature vector + caption text.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: np.ndarray, meta_df: pd.DataFrame):\n",
    "        assert len(features) == len(meta_df)\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.meta_df  = meta_df.reset_index(drop=True)\n",
    "\n",
    "        if \"abstracted\" not in self.meta_df.columns:\n",
    "            raise ValueError(\"meta_df must contain 'abstracted' column with caption text.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        eeg_feat = self.features[idx]            # [eeg_dim]\n",
    "        row = self.meta_df.iloc[idx]\n",
    "        caption = row[\"abstracted\"]\n",
    "        class_id = int(row[\"class_id\"])\n",
    "        return eeg_feat, caption, class_id\n",
    "\n",
    "train_ds = EEGCaptionDataset(eeg_train_feats, eeg_train_meta_full)\n",
    "val_ds   = EEGCaptionDataset(eeg_val_feats,   eeg_val_meta_full)\n",
    "test_ds  = EEGCaptionDataset(eeg_test_feats,  eeg_test_meta_full)\n",
    "\n",
    "print(\"Sizes — train:\", len(train_ds), \"val:\", len(val_ds), \"test:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG batch: torch.Size([128, 256])\n",
      "Example caption: Horse standing on grassy open field\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = config['batch_size']\n",
    "\n",
    "def collate_eeg_caption(batch):\n",
    "    \"\"\"\n",
    "    Collate function:\n",
    "        batch: list of (eeg_feat, caption, class_id)\n",
    "    \"\"\"\n",
    "    eeg_feats, captions, class_ids = zip(*batch)\n",
    "    eeg_tensor = torch.stack(eeg_feats, dim=0)      # [B, eeg_dim]\n",
    "    class_ids  = torch.tensor(class_ids, dtype=torch.long)\n",
    "    return eeg_tensor, list(captions), class_ids\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_eeg_caption,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_eeg_caption,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_eeg_caption,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "eeg_batch, cap_batch, cls_batch = next(iter(train_loader))\n",
    "print(\"EEG batch:\", eeg_batch.shape)\n",
    "print(\"Example caption:\", cap_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class LoRAAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple LoRA-style adapter on top of CLIP text embeddings:\n",
    "    out = x + B(A(x)), where A: d->r, B: r->d with r << d.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, rank: int = 32, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.A = nn.Linear(dim, rank, bias=False)\n",
    "        self.B = nn.Linear(rank, dim, bias=False)\n",
    "\n",
    "        # Initialize A small, B zero (so initial output is approx x)\n",
    "        nn.init.kaiming_uniform_(self.A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, d]\n",
    "        delta = self.B(self.A(x)) * (self.alpha / self.rank)\n",
    "        return x + delta\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "class EEGToCLIPModel(nn.Module):\n",
    "    \"\"\"\n",
    "    EEG → CLIP text space model with:\n",
    "      - Frozen CLIP text encoder\n",
    "      - LoRAAdapter on top of text embeddings\n",
    "      - Projection head: EEG features -> CLIP text embedding dim\n",
    "      - Contrastive (InfoNCE) loss between EEG embeddings and adapted text embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model: CLIPModel, eeg_feat_dim: int, lora_rank: int = 32, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.text_dim = clip_model.config.projection_dim\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # EEG projection: small MLP\n",
    "        self.eeg_proj = nn.Sequential(\n",
    "            nn.Linear(eeg_feat_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, self.text_dim)\n",
    "        )\n",
    "\n",
    "        # LoRA adapter on text embeddings\n",
    "        self.text_lora = LoRAAdapter(self.text_dim, rank=lora_rank, alpha=1.0)\n",
    "\n",
    "    def encode_text(self, captions, device):\n",
    "        \"\"\"\n",
    "        captions: list[str]\n",
    "        Returns: normalized text embeddings after LoRA adapter, shape [B, d]\n",
    "        \"\"\"\n",
    "        inputs = clip_processor(\n",
    "            text=captions,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_feats = self.clip_model.get_text_features(**inputs)  # [B, d]\n",
    "        text_feats = F.normalize(text_feats, dim=-1)\n",
    "\n",
    "        # Apply LoRA adapter (trainable)\n",
    "        text_feats = self.text_lora(text_feats)\n",
    "        text_feats = F.normalize(text_feats, dim=-1)\n",
    "        return text_feats\n",
    "\n",
    "    def encode_eeg(self, eeg_feats):\n",
    "        \"\"\"\n",
    "        eeg_feats: [B, eeg_feat_dim]\n",
    "        Returns: normalized EEG embeddings [B, d]\n",
    "        \"\"\"\n",
    "        z = self.eeg_proj(eeg_feats)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n",
    "\n",
    "    def forward(self, eeg_feats, captions, device):\n",
    "        \"\"\"\n",
    "        Compute InfoNCE loss between EEG embeddings and text embeddings.\n",
    "        \"\"\"\n",
    "        z_eeg = self.encode_eeg(eeg_feats)                # [B, d]\n",
    "        z_txt = self.encode_text(captions, device)        # [B, d]\n",
    "\n",
    "        # Similarity matrix [B, B]\n",
    "        logits = (z_eeg @ z_txt.T) / self.temperature\n",
    "\n",
    "        # Targets are diagonals (i matches i)\n",
    "        targets = torch.arange(z_eeg.size(0), device=device)\n",
    "\n",
    "        loss_i2t = F.cross_entropy(logits, targets)\n",
    "        loss_t2i = F.cross_entropy(logits.T, targets)\n",
    "\n",
    "        loss = 0.5 * (loss_i2t + loss_t2i)\n",
    "\n",
    "        return loss, z_eeg.detach(), z_txt.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 427008\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "model_eeg_clip = EEGToCLIPModel(\n",
    "    clip_model=clip_model,\n",
    "    eeg_feat_dim=eeg_feat_dim,\n",
    "    lora_rank=config['lora_rank'],\n",
    "    temperature=config['temperature']\n",
    ").to(device)\n",
    "\n",
    "print(\"Trainable params:\",\n",
    "      sum(p.numel() for p in model_eeg_clip.parameters() if p.requires_grad))\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_eeg_clip.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    betas=(0.9, 0.98),\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['epochs'],    # you can tune this\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 41.65it/s]\n",
      "Epoch 1/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss=4.8838 | val loss=4.8655\n",
      "New best model (val loss=4.8655)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2/10 [train]:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 120/122 [00:02<00:00, 50.48it/s]To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 45.03it/s]\n",
      "Epoch 2/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train loss=4.8165 | val loss=4.8622\n",
      "New best model (val loss=4.8622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 41.01it/s]\n",
      "Epoch 3/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.29it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train loss=4.7799 | val loss=4.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 45.40it/s]\n",
      "Epoch 4/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train loss=4.7454 | val loss=4.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 44.94it/s]\n",
      "Epoch 5/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 5/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train loss=4.7141 | val loss=4.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 45.38it/s]\n",
      "Epoch 6/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 6/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train loss=4.6874 | val loss=4.8851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 44.03it/s]\n",
      "Epoch 7/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 7/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 38.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train loss=4.6664 | val loss=4.8872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 8/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 45.10it/s]\n",
      "Epoch 8/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 8/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train loss=4.6501 | val loss=4.8891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [train]:   0%|                                                                                                                                                                  | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 9/10 [train]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 45.68it/s]\n",
      "Epoch 9/10 [val]:   0%|                                                                                                                                                                     | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 9/10 [val]: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train loss=4.6401 | val loss=4.8892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [train]:   0%|                                                                                                                                                                 | 0/122 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 10/10 [train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:02<00:00, 41.73it/s]\n",
      "Epoch 10/10 [val]:   0%|                                                                                                                                                                    | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 10/10 [val]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 39.48it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss=4.6342 | val loss=4.8894\n",
      "\n",
      "Loaded best model with val loss=4.8622\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from copy import deepcopy\n",
    "\n",
    "NUM_EPOCHS = config['epochs']\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # ---- Train ----\n",
    "    model_eeg_clip.train()\n",
    "    train_loss, train_count = 0.0, 0\n",
    "\n",
    "    for eeg_feats, captions, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [train]\"):\n",
    "        eeg_feats = eeg_feats.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss, _, _ = model_eeg_clip(eeg_feats, captions, device)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_eeg_clip.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = eeg_feats.size(0)\n",
    "        train_loss += loss.item() * bs\n",
    "        train_count += bs\n",
    "\n",
    "    train_loss /= max(1, train_count)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model_eeg_clip.eval()\n",
    "    val_loss, val_count = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for eeg_feats, captions, _ in tqdm(val_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS} [val]\"):\n",
    "            eeg_feats = eeg_feats.to(device, non_blocking=True)\n",
    "            loss, _, _ = model_eeg_clip(eeg_feats, captions, device)\n",
    "            bs = eeg_feats.size(0)\n",
    "            val_loss += loss.item() * bs\n",
    "            val_count += bs\n",
    "\n",
    "    val_loss /= max(1, val_count)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train loss={train_loss:.4f} | val loss={val_loss:.4f}\")\n",
    "\n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = deepcopy(model_eeg_clip.state_dict())\n",
    "        print(f\"New best model (val loss={val_loss:.4f})\")\n",
    "\n",
    "# Load best\n",
    "if best_state_dict is not None:\n",
    "    model_eeg_clip.load_state_dict(best_state_dict)\n",
    "    print(f\"\\nLoaded best model with val loss={best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total EEG-caption pairs: 26000\n",
      "Unique captions: 8331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing caption embedding bank: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 131/131 [00:01<00:00, 93.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption embedding bank: torch.Size([8331, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Build a bank of unique captions from EEG trials (train+val+test)\n",
    "all_meta = pd.concat(\n",
    "    [eeg_train_meta_full, eeg_val_meta_full, eeg_test_meta_full],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Use the caption text\n",
    "all_captions = all_meta[\"abstracted\"].tolist()\n",
    "print(\"Total EEG-caption pairs:\", len(all_captions))\n",
    "\n",
    "# Optionally deduplicate captions (many will repeat)\n",
    "unique_captions = sorted(set(all_captions))\n",
    "print(\"Unique captions:\", len(unique_captions))\n",
    "\n",
    "# Map caption -> index\n",
    "caption2idx = {c: i for i, c in enumerate(unique_captions)}\n",
    "\n",
    "# Precompute caption indices for each trial\n",
    "all_meta[\"caption_idx\"] = [caption2idx[c] for c in all_meta[\"abstracted\"]]\n",
    "\n",
    "# Compute text embedding bank for unique captions (with LoRA adapter)\n",
    "BATCH = 64\n",
    "text_emb_bank = []\n",
    "\n",
    "model_eeg_clip.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(unique_captions), BATCH), desc=\"Computing caption embedding bank\"):\n",
    "        batch_caps = unique_captions[i:i+BATCH]\n",
    "        z_txt = model_eeg_clip.encode_text(batch_caps, device)    # [b, d]\n",
    "        text_emb_bank.append(z_txt.cpu())\n",
    "\n",
    "text_emb_bank = torch.cat(text_emb_bank, dim=0)   # [N_caps, d]\n",
    "print(\"Caption embedding bank:\", text_emb_bank.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding EEG test set:   0%|                                                                                                                                                                | 0/41 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Encoding EEG test set: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 126.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG test embeddings: torch.Size([5200, 512])\n",
      "Caption idx test: (5200,)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Compute EEG embeddings on test set\n",
    "model_eeg_clip.eval()\n",
    "eeg_emb_list = []\n",
    "caption_idx_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for eeg_feats, captions, _ in tqdm(test_loader, desc=\"Encoding EEG test set\"):\n",
    "        eeg_feats = eeg_feats.to(device, non_blocking=True)\n",
    "        z_eeg = model_eeg_clip.encode_eeg(eeg_feats)  # [B, d]\n",
    "        eeg_emb_list.append(z_eeg.cpu())\n",
    "\n",
    "        # Map captions -> caption_idx via caption2idx\n",
    "        for c in captions:\n",
    "            caption_idx_list.append(caption2idx[c])\n",
    "\n",
    "eeg_emb_test = torch.cat(eeg_emb_list, dim=0)               # [N_test, d]\n",
    "caption_idx_test = np.array(caption_idx_list, dtype=np.int64)\n",
    "print(\"EEG test embeddings:\", eeg_emb_test.shape)\n",
    "print(\"Caption idx test:\", caption_idx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG → Caption Retrieval:\n",
      "  Recall@1: 0.0002\n",
      "  Recall@3: 0.0006\n",
      "  Recall@5: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def eeg_to_caption_retrieval(eeg_emb, text_bank, k=5):\n",
    "    \"\"\"\n",
    "    eeg_emb: [N, d]\n",
    "    text_bank: [M, d]\n",
    "    returns:\n",
    "        retrieved_indices: [N, k]\n",
    "    \"\"\"\n",
    "    # [N, M]\n",
    "    sims = eeg_emb @ text_bank.T\n",
    "    top_k_scores, top_k_idx = torch.topk(sims, k=k, dim=1)\n",
    "    return top_k_idx.numpy(), top_k_scores.numpy()\n",
    "\n",
    "K_VALUES = [1, 3, 5]\n",
    "\n",
    "retrieved_idx, retrieved_scores = eeg_to_caption_retrieval(\n",
    "    eeg_emb_test, text_emb_bank, k=max(K_VALUES)\n",
    ")\n",
    "\n",
    "def compute_recall_at_k_eeg(retrieved_idx, true_idx, k_values):\n",
    "    N = len(true_idx)\n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        topk = retrieved_idx[:, :k]\n",
    "        hits = 0\n",
    "        for i in range(N):\n",
    "            if true_idx[i] in topk[i]:\n",
    "                hits += 1\n",
    "        recalls[f\"Recall@{k}\"] = hits / N\n",
    "    return recalls\n",
    "\n",
    "recall_eeg = compute_recall_at_k_eeg(retrieved_idx, caption_idx_test, K_VALUES)\n",
    "\n",
    "print(\"EEG → Caption Retrieval:\")\n",
    "for k, v in recall_eeg.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CLIPScore distributions for EEG → Caption...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched CLIPScore: mean=0.0154, std=0.0249\n",
      "Mismatched CLIPScore: mean=0.0137, std=0.0244\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAIjCAYAAAA9VuvLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAb7RJREFUeJzt3Xd4FFX7//HPJpBCCjVVQghdOtIebICCdEFUwAIEAVEQRMCCha6IiiKKgD5K1EeKCCI2BBFEEekootKkKC0gJiFAAmTP7w9/2S876ckmuwnv13XlInPmzJl7Z84Oe+fMnLUZY4wAAAAAAA5e7g4AAAAAADwNiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAZKNNmzZq06aNu8MAJEk2m00PPfRQoe9n7dq1stlsWrt2baHvqyhVrVpVsbGx7g6jUMTGxiowMLBI9lWSjyNwORIlwIPFxcXJZrNl+fPjjz866mZX74EHHsjQ9nfffadevXrpqquuko+Pj8qWLauWLVtq0qRJOnHiRFG+TIf9+/dryJAhqlatmvz8/BQcHKzrrrtOr776qs6fP19o+/311181YcIEHTx4sND2kV8nTpzQmDFjVKdOHZUpU0YBAQFq2rSppkyZooSEBEe9Nm3aqH79+tm2NWHCBNlsNp06dcpRFhsb69RXgoOD1ahRI02fPl2pqalO23///ffq1KmTrrrqKvn5+alKlSrq1q2b5s+f79LX7E7px2HQoEGZrn/qqaccdS4/jrn1xRdfaMKECQWM8sqWnsTZbDb973//y7TOddddJ5vNluN7wpMcPXpUEyZM0I4dO9wdCoD/r5S7AwCQs0mTJikmJiZDeY0aNZyW27dvr379+mWoV6tWLaflcePGafLkyapWrZpiY2NVrVo1paSkaOvWrZo+fbreffdd7d+/37UvIgeff/657rzzTvn6+qpfv36qX7++Lly4oO+//16PPvqodu3apTfffLNQ9v3rr79q4sSJatOmjapWreq0buXKlYWyz9zYvHmzOnfurOTkZN17771q2rSpJGnLli16/vnntW7dOpfE5+vrq//+97+SpISEBC1ZskRjxozR5s2btXDhQknS4sWL1bt3bzVu3FgPP/ywypcvrwMHDmjdunV66623dPfddxc4Dk/h5+enJUuW6I033pCPj4/TugULFsjPz08pKSn5avuLL77QrFmzSJZcwM/PT/Pnz9e9997rVH7w4EH98MMP8vPzy7DN7t275eXlmX8jPnr0qCZOnKiqVauqcePG7g4HgEiUgGKhU6dOatasWY71atWqleFDg9WiRYs0efJk9erVS++//36GD4KvvPKKXnnllQLFm1cHDhxQnz59FB0drW+++UYRERGOdcOGDdO+ffv0+eefF2lM6azHp6gkJCTotttuk7e3t7Zv3646deo4rX/22Wf11ltvuWRfpUqVcuo3Q4cOVcuWLbVo0SK9/PLLioyM1IQJE1S3bl39+OOPGY5JfHy8S+LIDWOMUlJS5O/vX2j76Nixo5YvX64vv/xS3bt3d5T/8MMPOnDggG6//XYtWbKk0PaP3OncubOWL1+uU6dOqVKlSo7y+fPnKywsTDVr1tQ///zjtI2vr29RhwmgGPPMP6sAKDTjxo1TpUqV9Pbbb2eaBJQtW7bI/9r9wgsvKDk5WW+//bZTkpSuRo0aevjhhx3L8+bN00033aTQ0FD5+vqqbt26mj17dobtqlatqq5du2rlypVq3Lix/Pz8VLduXS1dutRRJy4uTnfeeackqW3bto5betKfzcjsGaX4+HgNHDhQYWFh8vPzU6NGjfTuu+861Tl48KBsNpteeuklvfnmm6pevbp8fX3VvHlzbd68OcdjMnfuXB05ckQvv/xyhiRJksLCwvT000/n2E5+eHl5OV5z+u2I+/fvV/PmzTPtM6GhoU7Ldrtdr776qho0aCA/Pz+FhISoY8eO2rJli6POpUuXNHnyZMdxqVq1qp588skMt/uln8OvvvpKzZo1k7+/v+bOnSvp32Ry5MiRioqKkq+vr2rUqKFp06bJbrc7tXHs2DH9/vvvunjxYq5e/1VXXaUbb7wxwy2FH3zwgRo0aJDp7Vzfffed7rzzTlWpUkW+vr6KiorSI4884nTLaGxsrGbNmiXJ+VbZvBy3dMuWLVP9+vXl6+urevXqacWKFRnqHDlyRPfdd5/CwsIc9d55550M9f766y/16NFDAQEBCg0N1SOPPJLhPGTmo48+ks1m07fffpth3dy5c2Wz2fTLL79Iko4fP64BAwaocuXK8vX1VUREhLp3716g2127d+8uX19fLV682Kl8/vz56tWrl7y9vTNsY3225uLFi5o4caJq1qwpPz8/VaxYUddff71WrVrlqJP+7M/hw4fVtWtXBQYG6qqrrnKcy507d+qmm25SQECAoqOjM/Sb06dPa8yYMWrQoIECAwMVHBysTp066aeffnLUWbt2rZo3by5JGjBggKNvxMXFOeps3LhRnTt3Vvny5RUQEKCGDRvq1VdfzfAajxw5oh49eigwMFAhISEaM2aM0tLSnOrY7XbNmDFD9erVk5+fn8LCwjRkyJAMiaUxRlOmTFHlypVVpkwZtW3bVrt27crsdAAlEiNKQDGQmJiY4XkIm82mihUrOpWlpKRk+txEcHCwfHx8tGfPHu3Zs0eDBg0q1Id+7Xa7hgwZoh49eqhLly451v/0009VrVo1XXvttblqf/bs2apXr55uvfVWlSpVSp9++qmGDh0qu92uYcOGOdXdu3evevfurQceeED9+/fXvHnzdOedd2rFihVq3769brzxRo0YMUIzZ87Uk08+qauvvlqSHP9anT9/Xm3atNG+ffv00EMPKSYmRosXL1ZsbKwSEhKcEjrp3w9tZ86c0ZAhQ2Sz2fTCCy+oZ8+e+uOPP1S6dOksX+Py5cvl7++vO+64I1fHxNXSb71M72PR0dFavXq1/vrrL1WuXDnbbQcOHKi4uDh16tRJgwYN0qVLl/Tdd9/pxx9/dIyMDho0SO+++67uuOMOjR49Whs3btTUqVP122+/6eOPP3Zqb/fu3brrrrs0ZMgQDR48WLVr19a5c+fUunVrHTlyREOGDFGVKlX0ww8/aOzYsTp27JhmzJjh2H7s2LF69913deDAgQy3Vmbl7rvv1sMPP6zk5GQFBgbq0qVLWrx4sUaNGpXpbXeLFy/WuXPn9OCDD6pixYratGmTXnvtNf3111+OD/JDhgzR0aNHtWrVKr3//vv5Om7Sv8+KLV26VEOHDlVQUJBmzpyp22+/XYcPH3acrxMnTug///mPY/KHkJAQffnllxo4cKCSkpI0cuRISf/255tvvlmHDx/WiBEjFBkZqffff1/ffPNNjseoS5cuCgwM1IcffqjWrVs7rVu0aJHq1avnSCpvv/127dq1S8OHD1fVqlUVHx+vVatW6fDhw7k+J1ZlypRR9+7dtWDBAj344IOSpJ9++km7du3Sf//7X/388885tjFhwgRNnTpVgwYNUosWLZSUlKQtW7Zo27Ztat++vaNeWlqaOnXqpBtvvFEvvPCCPvjgAz300EMKCAjQU089pXvuuUc9e/bUnDlz1K9fP7Vq1cpxu/Qff/yhZcuW6c4771RMTIxOnDihuXPnqnXr1vr1118VGRmpq6++WpMmTdK4ceN0//3364YbbpAkxzVx1apV6tq1qyIiIvTwww8rPDxcv/32mz777DOna05aWpo6dOigli1b6qWXXtLXX3+t6dOnq3r16o5jJP3bF+Pi4jRgwACNGDFCBw4c0Ouvv67t27dr/fr1jmvTuHHjNGXKFHXu3FmdO3fWtm3bdMstt+jChQv5OmdAsWMAeKx58+YZSZn++Pr6OtXNqp4ks2DBAmOMMZ988omRZGbMmOG0rd1uNydPnnT6uXjxYr7jvnjxorntttuMr6+v+eqrr7Ktm5iYaCSZ7t2757r9c+fOZSjr0KGDqVatmlNZdHS0kWSWLFnitL+IiAjTpEkTR9nixYuNJLNmzZoM7bZu3dq0bt3asTxjxgwjyfzvf/9zlF24cMG0atXKBAYGmqSkJGOMMQcOHDCSTMWKFc3p06cdddPPwaeffprtayxfvrxp1KhRtnWscdarVy/bOuPHjzeSzMmTJx1l/fv3NwEBAY7zvm/fPvPcc88Zm81mGjZs6Kj39ttvG0nGx8fHtG3b1jzzzDPmu+++M2lpaU77+Oabb4wkM2LEiAz7t9vtxhhjduzYYSSZQYMGOa0fM2aMkWS++eYbR1n6OVyxYoVT3cmTJ5uAgACzZ88ep/InnnjCeHt7m8OHDzu9RknmwIED2R4fY/59Hw0bNsycPn3a+Pj4mPfff98YY8znn39ubDabOXjwYKbHMbM+OXXqVGOz2cyhQ4ccZcOGDTOZ/debm+OWHp+Pj4/Zt2+fo+ynn34yksxrr73mKBs4cKCJiIgwp06dcmqrT58+pmzZso540/vzhx9+6Khz9uxZU6NGjSzfE5e76667TGhoqLl06ZKj7NixY8bLy8tMmjTJGGPMP//8YySZF198Mdu2cmvNmjVGklm8eLH57LPPjM1mc5zvRx991HEdyOw9ER0dbfr37+9YbtSokenSpUu2+0vvP88995yj7J9//jH+/v7GZrOZhQsXOsp///13I8mMHz/eUZaSkpLhfXLgwAHj6+vrOEbGGLN582YjycybN8+p7qVLl0xMTIyJjo42//zzj9O6y/tGepyXt2mMMU2aNDFNmzZ1LH/33XdGkvnggw+c6q1YscKpPD4+3vj4+JguXbo47efJJ580kpyOI1BScesdUAzMmjVLq1atcvr58ssvM9Tr3r17hnqrVq1S27ZtJUlJSUmSlGE0KTExUSEhIU4/2c28ZP7/cyJZ/Vy6dEnvvvuuWrdurR49emjNmjVZtpUeU1BQUK6Px+XPp6SPtrVu3Vp//PGHEhMTnepGRkbqtttucywHBwerX79+2r59u44fP57rfab74osvFB4errvuustRVrp0aY0YMULJyckZbkPq3bu3ypcv71hO/0vxH3/8ke1+kpKS8nRMCuLs2bOO816jRg09+eSTatWqldPIzn333acVK1aoTZs2+v777zV58mTdcMMNqlmzpn744QdHvSVLlshms2n8+PEZ9pN+m9kXX3whSRo1apTT+tGjR0tShufRYmJi1KFDB6eyxYsX64YbblD58uV16tQpx0+7du2UlpamdevWOerGxcXJGJOnkYvy5curY8eOWrBggaR/RwavvfZaRUdHZ1r/8j559uxZnTp1Stdee62MMdq+fXuO+8vNcUvXrl07Va9e3bHcsGFDBQcHO/qUMUZLlixRt27dZIxxOj4dOnRQYmKitm3bJunfcxEREeE0clmmTBndf//9OcYs/du/4+PjnaYR/+ijj2S329W7d29J/x4bHx8frV27NsOtXQV1yy23qEKFClq4cKGMMVq4cKHTezMn5cqV065du7R3794c614+E2K5cuVUu3ZtBQQEqFevXo7y2rVrq1y5ck7vb19fX8cEEmlpafr7778VGBio2rVrO85DdrZv364DBw5o5MiRKleunNM6a9+QlGGW0xtuuMEpnsWLF6ts2bJq3769U99o2rSpAgMDHdfrr7/+WhcuXNDw4cOd9pM+GglcCbj1DigGWrRokavJHCpXrqx27dpluT79g3dycrJTeWBgoOOe/JUrV+rFF1/Mdj+7du1SgwYNcownXbdu3TLsM11wcLAk6cyZM7lub/369Ro/frw2bNigc+fOOa1LTExU2bJlHcs1atTI8GEifRbAgwcPKjw8PNf7laRDhw6pZs2aGWbOSr9V79ChQ07lVapUcVpOT5py+sAYHBycp2NSEH5+fvr0008l/fuhLiYmJtPb6zp06KAOHTro3Llz2rp1qxYtWqQ5c+aoa9eu+v333xUaGqr9+/crMjJSFSpUyHJ/hw4dkpeXV4ZZG8PDw1WuXLkMxzCzGR/37t2rn3/+WSEhIZnuwxUTTNx9993q27evDh8+rGXLlumFF17Isu7hw4c1btw4LV++PMO5tSbvmcnNcUtn7VPSv/0qfb8nT55UQkKC3nzzzSxnikw/PocOHcr0PVK7du0c45D+nfiibNmyWrRokW6++WZJ/95217hxY8f7zNfXV9OmTdPo0aMVFham//znP+ratav69euX5/efVenSpXXnnXdq/vz5atGihf788888zcA4adIkde/eXbVq1VL9+vXVsWNH9e3bVw0bNnSql/7M2OXKli2rypUrZzh2ZcuWdeoD6c+evfHGGzpw4IDT80LW26czk34bbG6mOs8szsv7hvTveycxMTHDs4XpLu8bklSzZk2n9SEhIU5//AFKMhIl4AqSPilA+gPW6UqVKuVIsP76668c26lcubLmzZuXY70PPvhAX3/9dbZ/nQ4ODlZkZGSGmLKyf/9+3XzzzapTp45efvllRUVFycfHR1988YVeeeWVDA/yu1tmD5RL//7VPzt16tTRjh07dOHChUKfec/b2zvbBNuqTJkyuuGGG3TDDTeoUqVKmjhxor788kv1798/T/vN7K/hmclshju73a727dvrsccey3Qb65T4+XHrrbfK19dX/fv3V2pqqtPIweXS0tLUvn17nT59Wo8//rjq1KmjgIAAHTlyRLGxsS7vkzn1qfT93XvvvVmeE2sikF++vr7q0aOHPv74Y73xxhs6ceKE1q9fr+eee86p3siRI9WtWzctW7ZMX331lZ555hlNnTpV33zzjZo0aVKgGO6++27NmTNHEyZMUKNGjVS3bt1cb3vjjTdq//79+uSTT7Ry5Ur997//1SuvvKI5c+Y4jSBldcxz8/5+7rnn9Mwzz+i+++7T5MmTVaFCBXl5eWnkyJFF1jcuZ7fbFRoaqg8++CDT9Vn98QG4EpEoAVeQ2rVrq2bNmlq2bJlmzJihgICAfLVTrly5HL+V/d1339Xq1av10EMP6eWXX862bteuXfXmm29qw4YNatWqVbZ1P/30U6Wmpmr58uVOf1nP6va+ffv2yRjj9KF8z549kuS4FSu3H9ilfyc1+Pnnn2W3251GlX7//XfHelfo1q2bNmzYoCVLluTpVqKilj7SeezYMUlS9erV9dVXX+n06dNZjo5ER0fLbrdr7969TpNmnDhxQgkJCbk6htWrV1dycnKeEry88vf3V48ePfS///1PnTp1cpqC+nI7d+7Unj179O677zp9j9nlM6ely6qv5ea45VZISIiCgoKUlpaW4/GJjo7WL7/8kuE9snv37lzvr3fv3o73+2+//SZjjOO2u8tVr15do0eP1ujRo7V37141btxY06dPz/JLY3Pr+uuvV5UqVbR27VpNmzYtz9tXqFBBAwYM0IABA5ScnKwbb7xREyZMyPJLh/Pqo48+Utu2bfX22287lSckJDj1qez6hvTvH7hc0d+rV6+ur7/+Wtddd1220+ynvw/37t2ratWqOcpPnjzp8lsoAU/FM0rAFWbChAk6deqUBg8enOl0yTmNdOTGhQsXNGXKFA0ePFgzZ87Msf5jjz2mgIAADRo0SCdOnMiwfv/+/Y5pcNP/Ynp5nImJiVmOcB09etTpWZukpCS99957aty4seO2n/SEMSEhIcdYO3furOPHj2vRokWOskuXLum1115TYGBghtm/8uuBBx5QRESERo8e7UjsLhcfH68pU6a4ZF+5sXr16kzL0583Sr9V6/bbb5cxRhMnTsxQN/2cde7cWZKcZqaT5EioczNTYq9evbRhwwZ99dVXGdYlJCTo0qVLjuW8Tg9+uTFjxmj8+PF65plnsqyTWZ80xmQ6dXNWfS03xy23vL29Hd/1lNlI7cmTJx2/d+7cWUePHtVHH33kKDt37lyevty5Xbt2qlChghYtWqRFixapRYsWTrdLnjt3LsNMgdWrV1dQUJDTNOT5PU82m00zZ87U+PHj1bdv3zxt+/fffzstBwYGqkaNGrmaHj23vL29M5zDxYsX68iRI05lWfWNa665RjExMZoxY0aGdfm5Xvfq1UtpaWmaPHlyhnWXLl1y7KNdu3YqXbq0XnvtNaf9WN+3QEnGiBJQDHz55ZeOEYvLXXvttU5/6duzZ0+mf50NCwtzTHV7991365dfftHUqVO1adMm9enTRzExMTp79qx++eUXLViwQEFBQQW6B93Hx0fr169XSEhIrkZrqlevrvnz56t37966+uqr1a9fP9WvX18XLlzQDz/84Jh+W/r34W0fHx9169ZNQ4YMUXJyst566y2FhoY6RjUuV6tWLQ0cOFCbN29WWFiY3nnnHZ04ccIpsWrcuLG8vb01bdo0JSYmytfX1/E9TVb333+/5s6dq9jYWG3dulVVq1bVRx99pPXr12vGjBkum4ChfPny+vjjj9W5c2c1btxY9957r5o2bSpJ2rZtmxYsWJBh9O3kyZOZJk8xMTG65557ChRP9+7dFRMTo27duql69eo6e/asvv76a3366adq3ry5unXrJunf76Lq27evZs6cqb1796pjx46y2+367rvv1LZtWz300ENq1KiR+vfvrzfffFMJCQlq3bq1Nm3apHfffVc9evRwTD6SnUcffVTLly9X165dFRsbq6ZNm+rs2bPauXOnPvroIx08eNDx1/r8TA+erlGjRmrUqFG2derUqaPq1atrzJgxOnLkiIKDg7VkyZJM/+qefg5HjBihDh06yNvbW3369MnVccuL559/XmvWrFHLli01ePBg1a1bV6dPn9a2bdv09ddf6/Tp05KkwYMH6/XXX1e/fv20detWRURE6P3331eZMmVyva/SpUurZ8+eWrhwoc6ePauXXnrJaf2ePXt08803q1evXqpbt65KlSqljz/+WCdOnFCfPn0c9Qpynrp37+705cC5VbduXbVp00ZNmzZVhQoVtGXLFn300Ud5Pt7Z6dq1qyZNmqQBAwbo2muv1c6dO/XBBx84Xbulf6+D5cqV05w5cxQUFKSAgAC1bNlSMTExmj17trp166bGjRtrwIABioiI0O+//65du3Zl+seC7LRu3VpDhgzR1KlTtWPHDt1yyy0qXbq09u7dq8WLF+vVV1/VHXfc4fgOpqlTp6pr167q3Lmztm/fri+//DLL0VWgxCnCGfYA5FF204PLMo1sdvUun9463dq1a80dd9xhIiIiTOnSpU1wcLBp1qyZGT9+vDl27FjRvcjL7NmzxwwePNhUrVrV+Pj4mKCgIHPdddeZ1157zaSkpDjqLV++3DRs2ND4+fmZqlWrmmnTppl33nknwxTQ0dHRpkuXLuarr74yDRs2NL6+vqZOnTpm8eLFGfb91ltvmWrVqhlvb2+naZGt04MbY8yJEyfMgAEDTKVKlYyPj49p0KBBhil906cHz2xKZFmmD87O0aNHzSOPPGJq1apl/Pz8TJkyZUzTpk3Ns88+axITEx31WrduneX5v/nmm40x2U8PnpMFCxaYPn36mOrVqxt/f3/j5+dn6tata5566inHlOjpLl26ZF588UVTp04d4+PjY0JCQkynTp3M1q1bHXUuXrxoJk6caGJiYkzp0qVNVFSUGTt2rNN5Nub/zmFmzpw5Y8aOHWtq1KhhfHx8TKVKlcy1115rXnrpJXPhwgWn12jtG1nR/58ePDuZHcdff/3VtGvXzgQGBppKlSqZwYMHO6btvrxvXLp0yQwfPtyEhIQYm83mNFV4bo5bVvFZp7025t9+OmzYMBMVFWVKly5twsPDzc0332zefPNNp3qHDh0yt956qylTpoypVKmSefjhhx1TRec0PXi6VatWGUnGZrOZP//802ndqVOnzLBhw0ydOnVMQECAKVu2rGnZsqXTlOTG5P48XT49eHZyMz34lClTTIsWLUy5cuWMv7+/qVOnjnn22Wcz9J/M3iNZTclv7bMpKSlm9OjRJiIiwvj7+5vrrrvObNiwIdNryyeffGLq1q1rSpUqlaHvfP/996Z9+/YmKCjIBAQEmIYNGzpNCZ9VnOn91erNN980TZs2Nf7+/iYoKMg0aNDAPPbYY+bo0aOOOmlpaWbixImO2Nu0aWN++eWXTPsbUBLZjHHBfTYA4IGqVq2q+vXr67PPPnN3KAAAoJjhGSUAAAAAsCBRAgAAAAALEiUAAAAAsOAZJQAAAACwYEQJAAAAACxIlAAAAADAosR/4azdbtfRo0cVFBSUqy++BAAAAFAyGWN05swZRUZGyssr+zGjEp8oHT16VFFRUe4OAwAAAICH+PPPP1W5cuVs65T4RCkoKEjSvwcjODi4QG3Z7XadPHlSISEhOWagQGGhH8IT0A/hCeiH8AT0w+IlKSlJUVFRjhwhOyU+UUq/3S44ONgliVJKSoqCg4N5I8Bt6IfwBPRDeAL6ITwB/bB4ys0jOZxNAAAAALAgUQIAAAAACxIlAAAAALAo8c8oAQAAoOCMMbp06ZLS0tLcHYpHsdvtunjxolJSUnhGyQN4e3urVKlSLvlaIBIlAAAAZOvChQs6duyYzp075+5QPI4xRna7XWfOnOE7Oz1EmTJlFBERIR8fnwK1Q6IEAACALNntdh04cEDe3t6KjIyUj48PCcFl0kfaXDWKgfwzxujChQs6efKkDhw4oJo1axZolI9ECQAAAFm6cOGC7Ha7oqKiVKZMGXeH43FIlDyLv7+/SpcurUOHDunChQvy8/PLd1vcSAkAAIAc8fwNigtX9VV6PAAAAABYkCgBAAAAgAXPKAEAACBfBsZtLrJ9vR3bvMj25Wo2m00ff/yxevTo4dJ227Rpo8aNG2vGjBkubdeqsOL3dIwoAQAAoESKjY2VzWbTAw88kGHdsGHDZLPZFBsbm+v2Dh48KJvNph07drguyGLg2LFj6tSpk7vDKHIkSgAAACixoqKitHDhQp0/f95RlpKSovnz56tKlSpujKz4CA8Pl6+vb5brL168WITRFB0SJQAAAJRY11xzjaKiorR06VJH2dKlS1WlShU1adLEqe6KFSt0/fXXq1y5cqpYsaK6du2q/fv3O9bHxMRIkpo0aSKbzaY2bdo41r3zzjuqV6+efH19FRERoYceesip7VOnTum2225TmTJlVLNmTS1fvtxp/S+//KJOnTopMDBQYWFh6tu3r06dOuVYf/bsWfXr10+BgYGKiIjQ9OnTc3ztEyZMUOPGjTV37lzH9O69evVSYmKio87mzZvVvn17VapUSWXLllXr1q21bds2p3ZsNpuWLVsm6f9G1RYtWqTWrVvLz89PH3zwgQ4dOqRu3bqpfPnyCggIUL169fTFF1/kGKMnI1ECAABAiXbfffdp3rx5juV33nlHAwYMyFDv7NmzGjVqlLZs2aLVq1fLy8tLt912m+x2uyRp06ZNkqSvv/5ax44dcyRfc+fO1UMPPaT7779fO3fu1PLly1WjRg2ntidOnKhevXrp559/VufOnXXPPffo9OnTkqSEhATddNNNatKkibZs2aIVK1boxIkT6tWrl2P7Rx99VN9++60++eQTrVy5UmvXrs2Q0GRm3759+vDDD/Xpp59qxYoV2r59u4YOHepYf+bMGfXv31/ff/+9fvzxR9WsWVOdO3fWmTNnsm33iSee0MMPP6zffvtNHTp00LBhw5Samqp169Zp586dmjZtmgIDA3OMz5MxmQMAAABKtHvvvVdjx47VoUOHJEnr16/XwoULtXbtWqd6t99+u9PyO++8o5CQEP3666+qX7++QkJCJEkVK1ZUeHi4pH+/cHbq1KkaNWqUHn74Yce2zZs7Tz4RGxuru+66S5L03HPPaebMmdq0aZM6duyo119/XU2aNNFzzz3ntO+oqCjt2bNHkZGRevvtt/W///1PN998syTp3XffVeXKlXN87SkpKXrvvfd01VVXSZJee+01denSRdOnT1d4eLhuuukmp/pvvvmmypUrp2+//VZdu3bNst2RI0eqZ8+ejuXDhw/r9ttvV4MGDSRJ1apVyzE2T0eiBAAAgBItJCREXbp0UVxcnIwx6tKliypVqpSh3t69ezVu3Dht3LhRp06dcowkHT58WPXr18+07fj4eB09etSRwGSlYcOGjt8DAgIUHBys+Ph4SdJPP/2kNWvWZDoCs3//fp0/f14XLlxQy5YtHeUVKlRQ7dq1c3ztVapUcSRJktSqVSvZ7Xbt3r1b4eHhOnHihJ5++mmtXbtW8fHxSktL07lz53T48OFs223WrJnT8ogRI/Tggw9q5cqVateunW6//Xan11wckSgBAACgxLvvvvsczw3NmjUr0zrdunVTdHS03nrrLUVGRsput6t+/fq6cOFClu36+/vnav+lS5d2WrbZbI5ELDk5Wd26ddO0adMybBcREaF9+/blah/50b9/f/3999969dVXFR0dLV9fX7Vq1Srb1yz9m+xdbtCgQerQoYM+//xzrVy5UlOnTtX06dM1fPjwQou9sPGMEgAAAEq8jh076sKFC7p48aI6dOiQYf3ff/+t3bt36+mnn9bNN9+sq6++Wv/8849THR8fH0lSWlqaoywoKEhVq1bV6tWr8x3bNddco127dqlq1aqqUaOG009AQICqV6+u0qVLa+PGjY5t/vnnH+3ZsyfHtg8fPqyjR486ln/88Ud5eXk5RqPWr1+vESNGqHPnzo7JKC6fRCIvoqKi9MADD2jp0qUaPXq03nrrrXy14ykYUQIAFJ75vV3Tzt2LXNMOgCuWt7e3fvvtN8fvVuXLl1fFihX15ptvKiIiQocPH9YTTzzhVCc0NFT+/v5asWKFKleuLD8/PwUHB+vpp5/WQw89pLCwMHXq1ElnzpzR+vXrcz2aMmzYML311lu666679Nhjj6lChQrat2+fFi5cqP/+978KDAzUwIED9eijj6pixYoKDQ3VU089JS+vnMc8/Pz81L9/f7300ktKSkrSiBEj1KtXL8czVjVr1tT777+vZs2aKSkpSY8++miuR8kuN3LkSHXq1Em1atXSP//8ozVr1ujqq6/OczuehEQJAAAA+fJ2bPOcK3mQ4ODgLNd5eXlp4cKFGjFihOrXr6/atWtr5syZTlOAlypVSjNnztSkSZM0btw43XDDDVqzZo369eunixcvasaMGRozZowqVaqkO+64I9dxRUZGav369Xr88cd1yy23KDU1VdHR0erYsaMjGXrxxRcdt+gFBQVp9OjRTtN8Z6VGjRrq2bOnOnfurNOnT6tr16564403HOvffvtt3X///Y5p1J977jmNGTMm17GnS0tL07Bhw/TXX38pODhYHTt21CuvvJLndjyJzRhj3B1EYUpKSlLZsmWVmJiY7ZsjN+x2u+Lj4xUaGpqrDB4oDPRDeIJc90NGlFCIuB4WjZSUFB04cEAxMTHy8/NzdzgexxijS5cuqVSpUrLZbO4Ox8mECRO0bNky7dixw92hFKns+mxecgNGlAAAxcLAuM0F2r64/eUbAOBeJEoAgCuHK0a4GN0CgCsC49QAAABACTRhwoQr7rY7VyJRAgAAAAALEiUAAAAAsOAZJQAA8oKZ/ADgisCIEgAAAABYkCgBAAAAgAWJEgAAAABY8IwSAAAA8sdVz+zlRhE819emTRs1btxYM2bMKPR9udKECRO0bNkyl08FvnbtWrVt21b//POPypUr59K2L1dY8RcUI0oAAAAokWJjY2Wz2fTAAw9kWDds2DDZbDbFxsY6ypYuXarJkycXYYQZxcbGqkePHm6NoaiNGTNGq1evdncYGZAoAQAAoMSKiorSwoULdf78eUdZSkqK5s+frypVqjjVrVChgoKCgoo6xCteYGCgKlasmOX6CxcuFGE0/4dECQAAACXWNddco6ioKC1dutRRtnTpUlWpUkVNmjRxqtumTRuNHDnSsfzGG2+oZs2a8vPzU1hYmO644w6nusOHD9fIkSMVGhqq8PBwvfXWWzp79qwGDBigoKAg1ahRQ19++aVjm7S0NA0cOFAxMTHy9/dX7dq19eqrrzrWT5gwQe+++64++eQT2Ww22Ww2rV27VpL0119/6a677lKFChUUEBCgZs2aaePGjU7xv//++6patarKli2rPn366MyZM451drtdU6dOdey7UaNG+uijj5y2/+KLL1SrVi35+/urbdu2OnjwYI7H12azafbs2erUqZP8/f1VrVq1DO0+/vjjqlWrlsqUKaNq1arpmWee0cWLF51ed+PGjR3L6aNqzz77rCIjI1W7du0cz0dhIFECAABAiXbfffdp3rx5juV33nlHAwYMyHabLVu2aMSIEZo0aZJ2796tFStW6MYbb3Sq8+6776pSpUpav369HnroIT344IO68847de2112rbtm265ZZb1LdvX507d07Sv8lK5cqVtXjxYv36668aN26cnnzySX344YeS/r0FrVevXurYsaOOHTumY8eO6dprr1VycrJat26tI0eOaPny5frpp5/02GOPyW63O2LZv3+/li1bps8++0yfffaZvv32Wz3//POO9VOnTtV7772nOXPmaNeuXXrkkUd077336ttvv5Uk/fnnn+rZs6e6deumHTt2aNCgQXriiSdydXyfeeYZ3X777frpp590zz33qE+fPvrtt98c64OCghQXF6dff/1Vr776qt566y298sor2ba5evVq7d69W6tWrdJnn32Wq/PhakzmAAC4IgyM26zhJxLyvX3jqHIuiwVA0br33ns1duxYHTp0SJK0fv16LVy40DFak5nDhw8rICBAXbt2VVBQkKKjozOMQDVq1EhPP/20Ll26pLFjx2ratGmqVKmSBg8eLEkaN26cZs+erZ9//ln/+c9/VLp0aU2cONGxfUxMjDZs2KAPP/xQvXr1UmBgoPz9/ZWamqrw8HBHvbi4OJ08eVKbN29WhQoVJEk1atRwisVutysuLs5x62Dfvn21evVqPfvss0pNTdVzzz2nr7/+Wq1atZIkVatWTd9//73mzp2r1q1ba/bs2apevbqmT58uSapdu7Z27typadOm5Xh877zzTg0aNEiSNHnyZK1atUqvvfaa3njjDUnS008/7ahbtWpVjRkzRgsXLtRjjz2WZZsBAQH673//Kx8fH0n/jgLmdD5cjUQJAAAAJVpISIi6dOmiuLg4GWPUpUsXVapUKdtt2rdvr+joaFWrVk0dO3ZUx44dddttt6lMmTKOOg0bNnT87u3trYoVK6pBgwaOsrCwMElSfHy8o2zWrFl65513dPjwYZ0/f14XLlxwuu0sMzt27FCTJk0cSVJmqlat6vR8VUREhGO/+/bt07lz59S+fXunbS5cuOBINn777Te1bNnSaX16UpUTa71WrVo5zWC3aNEizZw5U/v371dycrIuXbqk4ODgbNts0KCBI0mScnc+XI1b7wAAAFDi3XfffYqLi9O7776r++67L8f6QUFB2rZtmxYsWKCIiAiNGzdOjRo1UkJCgqNO6dKlnbax2WxOZTabTZIct8gtXLhQY8aM0cCBA7Vy5Urt2LFDAwYMyHGyAn9//xzjzSyW9P0mJydLkj7//HPt2LHD8fPrr79meJ7I1TZs2KB77rlHnTt31meffabt27frqaeeyvE1BwQEOC3n5ny4GokSAAAASryOHTvqwoULunjxojp06JCrbUqVKqV27drphRde0M8//6yDBw/qm2++yXcM69ev17XXXquhQ4eqSZMmqlGjhvbv3+9Ux8fHR2lpaU5lDRs21I4dO3T69Ol87bdu3bry9fXV4cOHVaNGDaefqKgoSdLVV1+tTZs2OW33448/5qp9a70ff/xRV199tSTphx9+UHR0tJ566ik1a9ZMNWvWdNwCmVeuPh857q/QWgYAAAA8hLe3t2OCAW9v7xzrf/bZZ/rjjz904403qnz58vriiy9kt9sdM7DlR82aNfXee+/pq6++UkxMjN5//31t3rxZMTExjjpVq1bVV199pd27d6tixYoqW7as7rrrLj333HPq0aOHpk6dqoiICG3fvl2RkZG5uj0uKChIY8aM0SOPPCK73a7rr79eiYmJWr9+vYKDg9W/f3898MADmj59uh599FENGjRIW7duVVxcXK5e1+LFi9WsWTNdf/31+uCDD7Rp0ya9/fbbjtd8+PBhLVy4UM2bN9fnn3+ujz/+OM/HrjDOR07cmiitW7dOL774orZu3apjx47p448/zvAFW7/99psef/xxffvtt7p06ZLq1q2rJUuWZJj3HgCAwrTjz4QCbc9kECiR7l7k7gjyJKfnYi5Xrlw5LV26VBMmTFBKSopq1qypBQsWqF69evne/5AhQ7R9+3b17t1bNptNd911l4YOHeo0hfjgwYO1du1aNWvWTMnJyVqzZo3atGmjlStXavTo0ercubPjM/GsWbNyve/JkycrJCREU6dO1R9//KFy5crpmmuu0ZNPPilJqlKlipYsWaJHHnlEr732mlq0aKHnnnsuV7cpTpw4UQsXLtTQoUMVERGhBQsWqG7dupKkW2+9VY888ogeeughpaamqkuXLnrmmWc0YcKEPB27wjgfObEZY0yhtZ6DL7/8UuvXr1fTpk3Vs2fPDInS/v371aJFCw0cOFB33XWXgoODtWvXLv3nP/9RaGhorvaRlJSksmXLKjExMU9vjszY7XbFx8crNDRUXl7ctQj3oB/CE+S6H87v7Zod3r1IA+M2F7iZ4SeezrlSIcmQKBWzD5ieiOth0UhJSdGBAwcUExMjPz8/d4fjcYwxunTpkkqVKuV4JulKYrPZMh3scKfs+mxecgO3jih16tRJnTp1ynL9U089pc6dO+uFF15wlFWvXr0oQgMAAABwBfPYZ5Tsdrs+//xzPfbYY+rQoYO2b9+umJgYjR07NtuMNTU1VampqY7lpKQkR3uXfylXfmMyxhS4HaAg6IfwBLnvhy7666rdLpsKfgOEcVU8+WC37pv3cIFxPSwa6cc5/QcZpR+XK/X4eFrfSI8ns8//ebleeGyiFB8fr+TkZD3//POaMmWKpk2bphUrVqhnz55as2aNWrdunel2U6dOdfoir3QnT55USkpKgWKy2+1KTEyUMYYhfrgN/RCeINf90DvSNTuMj1do6dSc6+XgfKD7nm+N9w60FMRnXhG5xvWwaFy8eFF2u12XLl3SpUuX3B2OxzHGOGapuxJvvUuf5tuT+salS5dkt9v1999/Z5g2/cyZM7lux2MTpfRsr3v37nrkkUckSY0bN9YPP/ygOXPmZJkojR07VqNGjXIsJyUlKSoqSiEhIS55RslmsykkJIQLMtyGfghPkOt+mHbUNTsMDVX8xcMFbsY/ueBt5Fdo+XKWgtw9a4uscT0sGikpKTpz5oxKlSqlUqU89qOj21k/kMN9SpUqJS8vL1WsWDHDM0p5ec7OY3t7pUqVVKpUKceMGemuvvpqff/991lu5+vrK19f3wzlXl5eLrmI2mw2l7UF5Bf9EJ4gd/3QRbdieHm55LY5V9y+l19e1n3z/nUJroeFz8vLyzFSciWOmOTEGMPx8UBZXRvycq3w2KuKj4+Pmjdvrt27dzuV79mzR9HR0W6KCgAA4MqSPlJy7tw5N0cC5E56Xy3oKJ9bR5SSk5O1b98+x/KBAwe0Y8cOVahQQVWqVNGjjz6q3r1768Ybb1Tbtm21YsUKffrpp1q7dq37ggYAALiCeHt7q1y5cor//8/VlSlThpGTy1zp04N7EmOMzp07p/j4eJUrVy5XXyycHbcmSlu2bFHbtm0dy+nPFvXv319xcXG67bbbNGfOHE2dOlUjRoxQ7dq1tWTJEl1//fXuChkA4A7ze2v4iQR3RwFcscLDwyXJkSzh/6TPrnb5LYpwr3Llyjn6bEG4NVFq06ZNjlMJ3nfffbn6RmAAAAAUDpvNpoiICIWGhurixYvuDsejpM+uVrFiRZ6V8wClS5cu8EhSOo+dzAEAAACexdvb22UfQksKu92u0qVLy8/Pj0SphOFsAgAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWPCFswCAIrHjzwR3hwAAQK4xogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFqXcHQAAAFeCHX8mOC2/Frc5z228HdvcRdEAAHLCiBIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWLg1UVq3bp26deumyMhI2Ww2LVu2LMu6DzzwgGw2m2bMmFFk8QEAAAC4Mrk1UTp79qwaNWqkWbNmZVvv448/1o8//qjIyMgiigwAAADAlayUO3feqVMnderUKds6R44c0fDhw/XVV1+pS5cuRRQZAACFa/iJp/O+0fxyzst3L3JJLACAjNyaKOXEbrerb9++evTRR1WvXr1cbZOamqrU1FTHclJSkqMtu91e4HiMMQVuBygI+iE8Qe77oc3xm7nsd+SP3XoMr/DrANdDeAL6YfGSl/Pk0YnStGnTVKpUKY0YMSLX20ydOlUTJ07MUH7y5EmlpKQUKB673a7ExEQZY+TlxTwYcA/6IYrMty9kucouKdGrooz97+zv4fb+v1umzwcGuyy0K1W8d6ClIN49gXgIrofwBPTD4uXMmTO5ruuxidLWrVv16quvatu2bbLZcv9XyLFjx2rUqFGO5aSkJEVFRSkkJETBwQX7T9put8tmsykkJIQ3AtyGfogik3Y0y1V22WSTFJJ2TF4yuWruWHKCa+K6goWWL2cpCHVLHJ6C6yE8Af2wePHz88t1XY9NlL777jvFx8erSpUqjrK0tDSNHj1aM2bM0MGDBzPdztfXV76+vhnKvby8XNJ5bTaby9oC8ot+iKKRfQJkk+Qlk+tEyZbLeshahmPNNYDrITwC/bD4yMs58thEqW/fvmrXrp1TWYcOHdS3b18NGDDATVEBAAAAuBK4NVFKTk7Wvn37HMsHDhzQjh07VKFCBVWpUkUVK1Z0ql+6dGmFh4erdu3aRR0qAAAAgCuIWxOlLVu2qG3bto7l9GeL+vfvr7i4ODdFBQAAAOBK59ZEqU2bNjIm9/esZ/VcEgAAAAC4Ek+cAQAAAIAFiRIAAAAAWJAoAQAAAIAFiRIAAAAAWHjs9ygBADzHjj8TnJaNbDofGKxjyQl8kSwAoERiRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCilLsDAAAAubPjzwSn5dfiNudp+7djm7swGgAo2RhRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAACLUu4OAAAA5M/wE0/nbYP55TIvv3tRgWMBgJKGESUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsCBRAgAAAAALEiUAAAAAsHBrorRu3Tp169ZNkZGRstlsWrZsmWPdxYsX9fjjj6tBgwYKCAhQZGSk+vXrp6NHj7ovYAAAAABXBLcmSmfPnlWjRo00a9asDOvOnTunbdu26ZlnntG2bdu0dOlS7d69W7feeqsbIgUAAABwJSnlzp136tRJnTp1ynRd2bJltWrVKqey119/XS1atNDhw4dVpUqVoggRAAAAwBXIrYlSXiUmJspms6lcuXJZ1klNTVVqaqpjOSkpSZJkt9tlt9sLtH+73S5jTIHbAQqCfoiiY3P8Zi77PX05/QfFhz2r81VMrydcD+EJ6IfFS17OU7FJlFJSUvT444/rrrvuUnBwcJb1pk6dqokTJ2YoP3nypFJSUgoUg91uV2Jioowx8vJiHgy4B/0QRcY70vHr+UDn666RTRf8QiRJNpkiDQv5F+8dmMWK+KINxEW4HsIT0A+LlzNnzuS6brFIlC5evKhevXrJGKPZs2dnW3fs2LEaNWqUYzkpKUlRUVEKCQnJNsHKDbvdLpvNppCQEN4IcBv6IYpM2v9NnnMsOcFpVfpIkn/yYRKlYiS0fLksVoQWaRyuwvUQnoB+WLz4+fnluq7HJ0rpSdKhQ4f0zTff5Jjs+Pr6ytfXN0O5l5eXSzqvzWZzWVtAftEPUTT+LwHKLBmyXXYDHooHr6zOVTG+lnA9hCegHxYfeTlHHp0opSdJe/fu1Zo1a1SxYkV3hwQAAADgCuDWRCk5OVn79u1zLB84cEA7duxQhQoVFBERoTvuuEPbtm3TZ599prS0NB0/flySVKFCBfn4+LgrbAAAAAAlnFsTpS1btqht27aO5fRni/r3768JEyZo+fLlkqTGjRs7bbdmzRq1adOmqMIEAAAAcIVxa6LUpk0bGZP1ve3ZrQMAAACAwuLRzygBAADX2fFnQqblr8VtztX2b8c2d2E0AODZmJoDAAAAACxIlAAAAADAgkQJAAAAACxIlAAAAADAgkQJAAAAACxIlAAAAADAgunBAaCkmd87Q1FW00IDAIDMMaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgUcrdAQAAcjYwbnOu6w4/kVB4gQAAcIVgRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALNyaKK1bt07dunVTZGSkbDabli1b5rTeGKNx48YpIiJC/v7+ateunfbu3eueYAEAAABcMdyaKJ09e1aNGjXSrFmzMl3/wgsvaObMmZozZ442btyogIAAdejQQSkpKUUcKQAAAIArSSl37rxTp07q1KlTpuuMMZoxY4aefvppde/eXZL03nvvKSwsTMuWLVOfPn2KMlQAAAAAVxC3JkrZOXDggI4fP6527do5ysqWLauWLVtqw4YNWSZKqampSk1NdSwnJSVJkux2u+x2e4FistvtMsYUuB2gIOiHVyabTK7rGtkKMZL/20f6D4q/3PYvT7vucD2EJ6AfFi95OU8emygdP35ckhQWFuZUHhYW5liXmalTp2rixIkZyk+ePFngW/bsdrsSExNljJGXF/NgwD3oh1em0NKpOVf6/84HVinESP5lZNMFvxBJeUvi4Jly27/i4+MLOZK84XoIT0A/LF7OnDmT67oemyjl19ixYzVq1CjHclJSkqKiohQSEqLg4OACtW2322Wz2RQSEsIbAW5DP7wyxV88nOu6/sm5r5tf6SNJ/smHSZRKgDuTn81VvdBvymVfoc/8ggeTB1wP4Qnoh8WLn59frut6bKIUHh4uSTpx4oQiIiIc5SdOnFDjxo2z3M7X11e+vr4Zyr28vFzSeW02m8vaAvKLfnjlycstbkWVuNguuwEPVwavnM61G65JXA/hCeiHxUdezpHHns2YmBiFh4dr9erVjrKkpCRt3LhRrVq1cmNkAAAAAEo6t44oJScna9++fY7lAwcOaMeOHapQoYKqVKmikSNHasqUKapZs6ZiYmL0zDPPKDIyUj169HBf0AAAAABKvHwlSn/88YeqVatW4J1v2bJFbdu2dSynP1vUv39/xcXF6bHHHtPZs2d1//33KyEhQddff71WrFiRp3sLAQAAACCv8pUo1ahRQ61bt9bAgQN1xx135DtxadOmjYzJ+n5nm82mSZMmadKkSflqHwAAAADyI1/PKG3btk0NGzbUqFGjFB4eriFDhmjTpk2ujg0AAAAA3CJfiVLjxo316quv6ujRo3rnnXd07NgxXX/99apfv75efvllnTx50tVxAgAAAECRKdCsd6VKlVLPnj21ePFiTZs2Tfv27dOYMWMUFRWlfv366dixY66KEwAAAACKTIESpS1btmjo0KGKiIjQyy+/rDFjxmj//v1atWqVjh49qu7du7sqTgAAAAAoMvmazOHll1/WvHnztHv3bnXu3FnvvfeeOnfu7PgCp5iYGMXFxalq1aqujBUAAAAAikS+EqXZs2frvvvuU2xsrCIiIjKtExoaqrfffrtAwQEAAACAO+QrUVq1apWqVKniGEFKZ4zRn3/+qSpVqsjHx0f9+/d3SZAAAAAAUJTy9YxS9erVderUqQzlp0+fVkxMTIGDAgAAAAB3yleilNWXxCYnJ+f7y2cBAAAAwFPk6da7UaNGSZJsNpvGjRunMmXKONalpaVp48aNaty4sUsDBAAAAICilqdEafv27ZL+HVHauXOnfHx8HOt8fHzUqFEjjRkzxrURAgAAAEARy1OitGbNGknSgAED9Oqrryo4OLhQggKAK9L83lmuGn4ioejiAAAA+Zv1bt68ea6OAwAAAAA8Rq4TpZ49eyouLk7BwcHq2bNntnWXLl1a4MAAAAAAwF1ynSiVLVtWNpvN8TsAAAAAlFS5TpQuv92OW+8AAAAAlGT5+h6l8+fP69y5c47lQ4cOacaMGVq5cqXLAgMAAAAAd8lXotS9e3e99957kqSEhAS1aNFC06dPV/fu3TV79myXBggAAAAARS1fidK2bdt0ww03SJI++ugjhYeH69ChQ3rvvfc0c+ZMlwYIAAAAAEUtX4nSuXPnFBQUJElauXKlevbsKS8vL/3nP//RoUOHXBogAAAAABS1fCVKNWrU0LJly/Tnn3/qq6++0i233CJJio+P50toAQAAABR7+UqUxo0bpzFjxqhq1apq2bKlWrVqJenf0aUmTZq4NEAAAAAAKGq5nh78cnfccYeuv/56HTt2TI0aNXKU33zzzbrttttcFhwAAAAAuEO+EiVJCg8PV3h4uFNZixYtChwQAAAAALhbvhKls2fP6vnnn9fq1asVHx8vu93utP6PP/5wSXAAAAAA4A75SpQGDRqkb7/9Vn379lVERIRsNpur4wIAAAAAt8lXovTll1/q888/13XXXefqeAAAAADA7fI161358uVVoUIFV8cCAAAAAB4hX4nS5MmTNW7cOJ07d87V8QAAAACA2+Xr1rvp06dr//79CgsLU9WqVVW6dGmn9du2bXNJcAAAAADgDvlKlHr06OHiMAAAAADAc+QrURo/fryr4wAAAAAAj5HvL5xNSEjQRx99pP379+vRRx9VhQoVtG3bNoWFhemqq65yZYwAAMAD7PgzIdv1r8Vtznb927HNXRgNABSufCVKP//8s9q1a6eyZcvq4MGDGjx4sCpUqKClS5fq8OHDeu+991wdJwAAAAAUmXzNejdq1CjFxsZq79698vPzc5R37txZ69atc1lwAAAAAOAO+UqUNm/erCFDhmQov+qqq3T8+PECBwUAAAAA7pSvRMnX11dJSUkZyvfs2aOQkJACBwUAAAAA7pSvROnWW2/VpEmTdPHiRUmSzWbT4cOH9fjjj+v22293aYAAAAAAUNTylShNnz5dycnJCgkJ0fnz59W6dWvVqFFDQUFBevbZZ10dIwAAAAAUqXzNele2bFmtWrVK69ev108//aTk5GRdc801ateunavjAwAAAIAil+dEyW63Ky4uTkuXLtXBgwdls9kUExOj8PBwGWNks9kKI04AAAAAKDJ5uvXOGKNbb71VgwYN0pEjR9SgQQPVq1dPhw4dUmxsrG677TaXBpeWlqZnnnlGMTEx8vf3V/Xq1TV58mQZY1y6HwAAAAC4XJ5GlOLi4rRu3TqtXr1abdu2dVr3zTffqEePHnrvvffUr18/lwQ3bdo0zZ49W++++67q1aunLVu2aMCAASpbtqxGjBjhkn0AAAAAgFWeRpQWLFigJ598MkOSJEk33XSTnnjiCX3wwQcuC+6HH35Q9+7d1aVLF1WtWlV33HGHbrnlFm3atMll+wAAAAAAqzyNKP3888964YUXslzfqVMnzZw5s8BBpbv22mv15ptvas+ePapVq5Z++uknff/993r55Zez3CY1NVWpqamO5fTve7Lb7bLb7QWKx263yxhT4HaAgqAflmRZP+NpslnnDkY2xw+Qzqbsb4139XWL6yE8Af2weMnLecpTonT69GmFhYVluT4sLEz//PNPXprM1hNPPKGkpCTVqVNH3t7eSktL07PPPqt77rkny22mTp2qiRMnZig/efKkUlJSChSP3W5XYmKijDHy8srXzOpAgdEPSzDvyCxXnQ8MLsJAcmZk0wW/f79gPKcPx7hyhJZOzXZ9fHy8S/fH9RCegH5YvJw5cybXdfOUKKWlpalUqaw38fb21qVLl/LSZLY+/PBDffDBB5o/f77q1aunHTt2aOTIkYqMjFT//v0z3Wbs2LEaNWqUYzkpKUlRUVEKCQlRcHDBPmjY7XbZbDaFhITwRoDb0A9LsLSjWa46lpxQdHHkQvpIkn/yYRIlOMQH+Ga7PjQ01KX743oIT0A/LF78/PxyXTdPiZIxRrGxsfL1zfxCePktb67w6KOP6oknnlCfPn0kSQ0aNNChQ4c0derULBMlX1/fTOPz8vJySee12WwuawvIL/phSZV1wuGJyYjtshvwACnnW0QL45rF9RCegH5YfOTlHOUpUcoqObmcq2a8k6Rz585leDHe3t7cAwoAAACgUOUpUZo3b15hxZGpbt266dlnn1WVKlVUr149bd++XS+//LLuu+++Io0DAAAAwJUlT4lSUXvttdf0zDPPaOjQoYqPj1dkZKSGDBmicePGuTs0AAAAACWYRydKQUFBmjFjhmbMmOHuUAAAAABcQXjiDAAAAAAsSJQAAAAAwIJECQAAAAAsSJQAAAAAwIJECQAAAAAsSJQAAAAAwMKjpwcHAAAlx8C4zQXa/u3Y5i6KBAByxogSAAAAAFiQKAEAAACABbfeAYArzO/t7ggAAIALMaIEAAAAABYkSgAAAABgwa13AADAJYafeLrAbbwWNsUFkQBAwTGiBAAAAAAWJEoAAAAAYEGiBAAAAAAWJEoAAAAAYEGiBAAAAAAWJEoAAAAAYMH04ABQBHb8meDuEAAAQB4wogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFiRKAAAAAGBBogQAAAAAFh6fKB05ckT33nuvKlasKH9/fzVo0EBbtmxxd1gAAAAASrBS7g4gO//884+uu+46tW3bVl9++aVCQkK0d+9elS9f3t2hAQAAACjBPDpRmjZtmqKiojRv3jxHWUxMjBsjAgAAAHAl8OhEafny5erQoYPuvPNOffvtt7rqqqs0dOhQDR48OMttUlNTlZqa6lhOSkqSJNntdtnt9gLFY7fbZYwpcDtAQdAPPZUt27Umh/XFjZHN8QO4kk0my3XW6x7XQ3gC+mHxkpfz5NGJ0h9//KHZs2dr1KhRevLJJ7V582aNGDFCPj4+6t+/f6bbTJ06VRMnTsxQfvLkSaWkpBQoHrvdrsTERBlj5OXl8Y93oYSiH3oo78hsV58PDC6iQIqGkU0X/EIkZf/BFsir0NKpWa6Lj493WuZ6CE9APyxezpw5k+u6NmOMx/4P5+Pjo2bNmumHH35wlI0YMUKbN2/Whg0bMt0msxGlqKgo/fPPPwoOLtgHFbvdrpMnTyokJIQ3AtyGfuihFt6d7eqf/kwomjiKiJFN5wOryD/5MIkSXGpW2KQs173Zr5nTMtdDeAL6YfGSlJSk8uXLKzExMcfcwKNHlCIiIlS3bl2nsquvvlpLlizJchtfX1/5+vpmKPfy8nJJ57XZbC5rC8gv+qEnyj5ZKInJhO2yG/AAV8nuds7MrnlcD+EJ6IfFR17OkUefzeuuu067d+92KtuzZ4+io6PdFBEAAACAK4FHJ0qPPPKIfvzxRz333HPat2+f5s+frzfffFPDhg1zd2gAAAAASjCPTpSaN2+ujz/+WAsWLFD9+vU1efJkzZgxQ/fcc4+7QwMAAABQgnn0M0qS1LVrV3Xt2tXdYQAAAAC4gnj0iBIAAAAAuIPHjygBAABI0sC4zU7LNhmFlk5V/MXDufry47djmxdWaABKIEaUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCWe8AIBess21ZDT+RUDSBAACAIsGIEgAAAABYMKIEAAA8xvATT+e6rpFN5wOryD/5sGwyTuteC5vi6tAAXGEYUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALBgenAAV7b5vXNVjS+UBQDgysKIEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYkCgBAAAAgAWJEgAAAABYFKtE6fnnn5fNZtPIkSPdHQoAAACAEqzYJEqbN2/W3Llz1bBhQ3eHAgAAAKCEKxaJUnJysu655x699dZbKl++vLvDAQAAAFDClXJ3ALkxbNgwdenSRe3atdOUKVOyrZuamqrU1FTHclJSkiTJbrfLbrcXKA673S5jTIHbAQqCfuhqtlzVMrmsd6Uwsjl+AHfJrh/aZDKUcd1EYeD/5eIlL+fJ4xOlhQsXatu2bdq8eXOu6k+dOlUTJ07MUH7y5EmlpKQUKBa73a7ExEQZY+TlVSwG41AC0Q9dzDsyV9XOBwYXciDFi5FNF/xCJGX+gRQoCtn1w9DSqRnqP71gfYH3OeLmmgVuAyUL/y8XL2fOnMl1XY9OlP788089/PDDWrVqlfz8/HK1zdixYzVq1CjHclJSkqKiohQSEqLg4IJ90LHb7bLZbAoJCeGNALehH7pY2tFcVTuWnFC4cRQz6X/B908+TKIEt8muH8YH+BbKPkNDQwulXRRf/L9cvOQ2p5A8PFHaunWr4uPjdc011zjK0tLStG7dOr3++utKTU2Vt7e30za+vr7y9c14cfTy8nJJ57XZbC5rC8gv+qEr5e5DPslARrbLbnwC3CWrflhYt4Vy3UVm+H+5+MjLOfLoROnmm2/Wzp07ncoGDBigOnXq6PHHH8+QJAG4wszv7e4IAABACeXRiVJQUJDq16/vVBYQEKCKFStmKAcAAAAAV2F8EAAAAAAsPHpEKTNr1651dwgAAAAASjhGlAAAAADAgkQJAAAAACyK3a13AJBXO/5McHcIAACgmGFECQAAAAAsSJQAAAAAwIJb7wB4vIFxmzMtH34ioWgDAQAAVwxGlAAAAADAgkQJAAAAACxIlAAAAADAgkQJAAAAACxIlAAAAADAgkQJAAAAACxIlAAAAADAgkQJAAAAACxIlAAAAADAopS7AwAAACguBsZtLtD2b8c2d1EkAAobI0oAAAAAYMGIEgAAKHGGn3i6wG28FjbFBZEAKK4YUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALAo5e4AAJR8A+M2uzsEAACAPGFECQAAAAAsSJQAAAAAwIJECQAAAAAsSJQAAAAAwIJECQAAAAAsmPUOQJEbfuJpd4cAADly1bXqtbApLmkHQNFiRAkAAAAALEiUAAAAAMDC4xOlqVOnqnnz5goKClJoaKh69Oih3bt3uzssAAAAACWYxydK3377rYYNG6Yff/xRq1at0sWLF3XLLbfo7Nmz7g4NAAAAQAnl8ZM5rFixwmk5Li5OoaGh2rp1q2688UY3RQUAAACgJPP4RMkqMTFRklShQoVM16empio1NdWxnJSUJEmy2+2y2+0F2rfdbpcxpsDtAAVRHPuhTcZp2cjmpkjgKkY2xw/gLsWlH15+DSxO127kTnH8f/lKlpfzVKwSJbvdrpEjR+q6665T/fr1M60zdepUTZw4MUP5yZMnlZKSUuD9JyYmyhgjLy+Pv2sRJVRx7IehpVOdls8HVnFTJHAVI5su+IVIypgIA0WluPTD+87+1/F7/OKF+W+o9WMuiAauVhz/X76SnTlzJtd1i1WiNGzYMP3yyy/6/vvvs6wzduxYjRo1yrGclJSkqKgohYSEKDg4uED7t9vtstlsCgkJ4Y0AtymO/TD+4mGnZf/kw1nURHGR/hd8/+TDHv0BFSVbceyHoeXLFWDjUJfFAdcpjv8vX8n8/PxyXbfYJEoPPfSQPvvsM61bt06VK1fOsp6vr698fX0zlHt5ebmk89psNpe1BeRXceuH1ttiissHGmTPdtmNT4C7FLd+6FWQOIvJNf9KVNz+X76S5eUceXyiZIzR8OHD9fHHH2vt2rWKiYlxd0gAAAD5suPPhHxv+1rcZr0d29x1wQDIlscnSsOGDdP8+fP1ySefKCgoSMePH5cklS1bVv7+/m6ODgAAAEBJ5PHjg7Nnz1ZiYqLatGmjiIgIx8+iRYvcHRoAAACAEsrjR5SMKR73HAMAAAAoOTx+RAkAAAAAihqJEgAAAABYePytdwA8yPze+dps+IkE18YBAABQyEiUAORoYNxmSSQ8AADgysGtdwAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABYkSgAAAABgQaIEAAAAABal3B0AAAAAcjb8xNPS/HIFa+TuRS6JBbgSkCgBV4r5vfO96fATCa6LAwAAoBjg1jsAAAAAsGBECQAAoJjY8WdCgbZv7JIogCsDI0oAAAAAYEGiBAAAAAAW3HoHeLoMkzDYJO9IKe2oJOOOiAAAAEo8RpQAAAAAwIJECQAAAAAsuPUOuAIUdJYkAACAKw2JEgAAwBViYNzmAm3/dmxzF0UCeD5uvQMAAAAACxIlAAAAALAgUQIAAAAACxIlAAAAALBgMgcAAIArxPATTxesgfnl/v337kUFjgXwdCRKAAAAyJX0r5t4rQCz5zFzHooLbr0DAAAAAAtGlIBCVtDvrBh+IkGNo8q5JhgAAADkCiNKAAAAAGBBogQAAAAAFtx6BxSm+b01/ESCu6MAAABAHpEoAQAAIE8KNM14+hTjEtOMw6ORKKFkmd/bNe142IU7fTpWSTKy6XxgsI4lJ8gm476gAAAASjASJSAzrkq4AAAAUCyRKKHEu3w0Jj+YmhsAAODKQ6IEAACAIuP0B8xpHdwWh9VrYVNyVe/t2OaFHAk8RbGYHnzWrFmqWrWq/Pz81LJlS23atMndIQEAAAAowTx+RGnRokUaNWqU5syZo5YtW2rGjBnq0KGDdu/erdDQUHeH5/EGxm0u0Pb81aTgt+4BAADPl9uZ/HZMc17+d5KlKjqWfFivh03OcXs+WxUfHp8ovfzyyxo8eLAGDBggSZozZ44+//xzvfPOO3riiSfcHF0+5HGSgKw+pOd2eLgkyEuyx3cWAQAAT8YfsYsPj06ULly4oK1bt2rs2LGOMi8vL7Vr104bNmzIdJvU1FSlpqY6lhMTEyVJCQkJstvtBYrHbrcrKSlJPj4+8vLK512L5y7lqfqZlLRMyy+eP5O//edRQkJCzpU+Gpjt6p1HEgsUQ2we6hbNUXEvI5tSvC/qUkoa04PDbeiH8AT0Q3iCy/th7KGxOW+QC2+FPpnlulx9NnOlHD7n5dodb7umnQJKSkqSJBmT8zXDoxOlU6dOKS0tTWFhYU7lYWFh+v333zPdZurUqZo4cWKG8ujo6EKJ0X1WF8le3h9aJLsBAACAQ9af84rtZ7PBS90dgZMzZ86obNmy2dbx6EQpP8aOHatRo0Y5lu12u06fPq2KFSvKZrMVqO2kpCRFRUXpzz//VHBwcEFDBfKFfghPQD+EJ6AfwhPQD4sXY4zOnDmjyMjIHOt6dKJUqVIleXt768SJE07lJ06cUHh4eKbb+Pr6ytfX16msXLlyLo0rODiYNwLcjn4IT0A/hCegH8IT0A+Lj5xGktJ59PTgPj4+atq0qVav/r/hR7vdrtWrV6tVq1ZujAwAAABASebRI0qSNGrUKPXv31/NmjVTixYtNGPGDJ09e9YxCx4AAAAAuJrHJ0q9e/fWyZMnNW7cOB0/flyNGzfWihUrMkzwUBR8fX01fvz4DLf2AUWJfghPQD+EJ6AfwhPQD0sum8nN3HgAAAAAcAXx6GeUAAAAAMAdSJQAAAAAwIJECQAAAAAsSJQAAAAAwIJE6TKnT5/WPffco+DgYJUrV04DBw5UcnJyttu8+eabatOmjYKDg2Wz2ZSQkOCSdnHlyk9/SUlJ0bBhw1SxYkUFBgbq9ttvz/BFzTabLcPPwoULC/OloJiZNWuWqlatKj8/P7Vs2VKbNm3Ktv7ixYtVp04d+fn5qUGDBvriiy+c1htjNG7cOEVERMjf31/t2rXT3r17C/MloARwdT+MjY3NcO3r2LFjYb4ElAB56Ye7du3S7bffrqpVq8pms2nGjBkFbhOegUTpMvfcc4927dqlVatW6bPPPtO6det0//33Z7vNuXPn1LFjRz355JMubRdXrvz0l0ceeUSffvqpFi9erG+//VZHjx5Vz549M9SbN2+ejh075vjp0aNHIb0KFDeLFi3SqFGjNH78eG3btk2NGjVShw4dFB8fn2n9H374QXfddZcGDhyo7du3q0ePHurRo4d++eUXR50XXnhBM2fO1Jw5c7Rx40YFBASoQ4cOSklJKaqXhWKmMPqhJHXs2NHp2rdgwYKieDkopvLaD8+dO6dq1arp+eefV3h4uEvahIcwMMYY8+uvvxpJZvPmzY6yL7/80thsNnPkyJEct1+zZo2RZP755x+XtosrS376S0JCgildurRZvHixo+y3334zksyGDRscZZLMxx9/XGixo3hr0aKFGTZsmGM5LS3NREZGmqlTp2Zav1evXqZLly5OZS1btjRDhgwxxhhjt9tNeHi4efHFFx3rExISjK+vr1mwYEEhvAKUBK7uh8YY079/f9O9e/dCiRclU1774eWio6PNK6+84tI24T6MKP1/GzZsULly5dSsWTNHWbt27eTl5aWNGzd6XLsomfLTX7Zu3aqLFy+qXbt2jrI6deqoSpUq2rBhg1PdYcOGqVKlSmrRooXeeecdGb5GDZIuXLigrVu3OvUhLy8vtWvXLkMfSrdhwwan+pLUoUMHR/0DBw7o+PHjTnXKli2rli1bZtkmrmyF0Q/TrV27VqGhoapdu7YefPBB/f33365/ASgR8tMP3dEmikYpdwfgKY4fP67Q0FCnslKlSqlChQo6fvy4x7WLkik//eX48ePy8fFRuXLlnMrDwsKctpk0aZJuuukmlSlTRitXrtTQoUOVnJysESNGuPx1oHg5deqU0tLSFBYW5lQeFham33//PdNtjh8/nmn99D6X/m92dYDLFUY/lP697a5nz56KiYnR/v379eSTT6pTp07asGGDvL29Xf9CUKzlpx+6o00UjRKfKD3xxBOaNm1atnV+++23IooGVypP6IfPPPOM4/cmTZro7NmzevHFF0mUAJRoffr0cfzeoEEDNWzYUNWrV9fatWt18803uzEyAJ6uxCdKo0ePVmxsbLZ1qlWrpvDw8AwP1F26dEmnT5/O8sG83CisdlG8FGY/DA8P14ULF5SQkOA0qnTixIls+1jLli01efJkpaamytfXN9evBSVPpUqV5O3tnWGmxOz6UHh4eLb10/89ceKEIiIinOo0btzYhdGjpCiMfpiZatWqqVKlStq3bx+JEjLITz90R5soGiX+GaWQkBDVqVMn2x8fHx+1atVKCQkJ2rp1q2Pbb775Rna7XS1btsz3/gurXRQvhdkPmzZtqtKlS2v16tWOst27d+vw4cNq1apVljHt2LFD5cuXJ0mCfHx81LRpU6c+ZLfbtXr16iz7UKtWrZzqS9KqVasc9WNiYhQeHu5UJykpSRs3bsy2X+LKVRj9MDN//fWX/v77b6cEHkiXn37ojjZRRNw9m4Qn6dixo2nSpInZuHGj+f77703NmjXNXXfd5Vj/119/mdq1a5uNGzc6yo4dO2a2b99u3nrrLSPJrFu3zmzfvt38/fffuW4XuFx++uEDDzxgqlSpYr755huzZcsW06pVK9OqVSvH+uXLl5u33nrL7Ny50+zdu9e88cYbpkyZMmbcuHFF+trguRYuXGh8fX1NXFyc+fXXX839999vypUrZ44fP26MMaZv377miSeecNRfv369KVWqlHnppZfMb7/9ZsaPH29Kly5tdu7c6ajz/PPPm3LlyplPPvnE/Pzzz6Z79+4mJibGnD9/vshfH4oHV/fDM2fOmDFjxpgNGzaYAwcOmK+//tpcc801pmbNmiYlJcUtrxGeL6/9MDU11Wzfvt1s377dREREmDFjxpjt27ebvXv35rpNeCYSpcv8/fff5q677jKBgYEmODjYDBgwwJw5c8ax/sCBA0aSWbNmjaNs/PjxRlKGn3nz5uW6XeBy+emH58+fN0OHDjXly5c3ZcqUMbfddps5duyYY/2XX35pGjdubAIDA01AQIBp1KiRmTNnjklLSyvKlwYP99prr5kqVaoYHx8f06JFC/Pjjz861rVu3dr079/fqf6HH35oatWqZXx8fEy9evXM559/7rTebrebZ555xoSFhRlfX19z8803m927dxfFS0Ex5sp+eO7cOXPLLbeYkJAQU7p0aRMdHW0GDx7Mh1PkKC/9MP3/ZetP69atc90mPJPNGOYHBgAAAIDLlfhnlAAAAAAgr0iUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkAAAAALEiUAAAAAMCCRAkA4HYXLlxQjRo19MMPPxTpftu0aaORI0cW6T4zM2HCBDVu3LhAbRw8eFA2m007duyQJK1du1Y2m00JCQkFjs9ms2nZsmUFamPFihVq3Lix7HZ7geMBgKJAogQAhez48eMaPny4qlWrJl9fX0VFRalbt25avXq1o07VqlU1Y8aMTLe3fgBOX07/qVixom655RZt377dsc2BAwd09913KzIyUn5+fqpcubK6d++u33//vTBfar7NmTNHMTExuvbaa53K16xZo86dO6tixYoqU6aM6tatq9GjR+vIkSMu2e/SpUs1efJkl7SVnY8//lj/+c9/VLZsWQUFBalevXpOCdqYMWOc+kN+REVF6dixY6pfv34Bo83o2LFj6tSpk6SM/TG3OnbsqNKlS+uDDz5weXwAUBhIlACgEB08eFBNmzbVN998oxdffFE7d+7UihUr1LZtWw0bNqxAbX/99dc6duyYvvrqKyUnJ6tTp05KSEjQxYsX1b59eyUmJmrp0qXavXu3Fi1apAYNGrhkdCErFy9ezNd2xhi9/vrrGjhwoFP53Llz1a5dO4WHh2vJkiX69ddfNWfOHCUmJmr69OmuCFkVKlRQUFCQS9rKyurVq9W7d2/dfvvt2rRpk7Zu3apnn33W6XgFBgaqYsWKBdqPt7e3wsPDVapUqYKG7HDhwgVJUnh4uHx9fQvcXmxsrGbOnFngdgCgSBgAQKHp1KmTueqqq0xycnKGdf/884/j9+joaPPKK69k2saBAweMJLN9+/ZMl40xZv369UaSWbFihdm+fbuRZA4ePJhtbH/++afp06ePKV++vClTpoxp2rSp+fHHHx3r33jjDVOtWjVTunRpU6tWLfPee+85bS/JvPHGG6Zbt26mTJkyZvz48cYYY5YtW2aaNGlifH19TUxMjJkwYYK5ePFilnFs3rzZeHl5maSkJKfYfHx8zMiRIzPd5vJj99FHH5m6desaHx8fEx0dbV566SWnurNmzTI1atQwvr6+JjQ01Nx+++2Oda1btzYPP/ywYzk6Oto8++yzZsCAASYwMNBERUWZuXPnOrV3+PBhc+edd5qyZcua8uXLm1tvvdUcOHAgy9f38MMPmzZt2mS53hhjxo8fbxo1auRY7t+/v+nevbt59tlnTWhoqClbtqyZOHGiuXjxohkzZowpX768ueqqq8w777zj2MbaL9asWWMkOY7VqVOnTJ8+fUxkZKTx9/c39evXN/Pnz3eKo3Xr1mbYsGHm4YcfNhUrVnTELcl8/PHHjt8v/2ndurX59ttvTalSpcyxY8cyvPbrr7/esXzo0CEjyezbty/b4wEAnoARJQAoJKdPn9aKFSs0bNgwBQQEZFhfrlw5l+3L399f0r8jACEhIfLy8tJHH32ktLS0TOsnJyerdevWOnLkiJYvX66ffvpJjz32mOP5kY8//lgPP/ywRo8erV9++UVDhgzRgAEDtGbNGqd2JkyYoNtuu007d+7Ufffdp++++079+vXTww8/rF9//VVz585VXFycnn322Sxj/+6771SrVi2nkZ3FixfrwoULeuyxxzLdJv3Ybd26Vb169VKfPn20c+dOTZgwQc8884zi4uIkSVu2bNGIESM0adIk7d69WytWrNCNN96Y7bGcPn26mjVrpu3bt2vo0KF68MEHtXv3bkn/jpp16NBBQUFB+u6777R+/XoFBgaqY8eOjtEXq/DwcO3atUu//PJLtvu1+uabb3T06FGtW7dOL7/8ssaPH6+uXbuqfPny2rhxox544AENGTJEf/31V67aS0lJUdOmTfX555/rl19+0f3336++fftq06ZNTvXeffdd+fj4aP369ZozZ06GdtLrp49oLl26VDfeeKOqVaum999/31Hv4sWL+uCDD3Tfffc5yqpUqaKwsDB99913eToWAOAW7s7UAKCk2rhxo5Fkli5dmmPdgowo/fPPP+a2224zgYGB5vjx48YYY15//XVTpkwZExQUZNq2bWsmTZpk9u/f72hz7ty5JigoyPz999+Z7vPaa681gwcPdiq78847TefOnR3LkjKM+Nx8883mueeecyp7//33TURERJav/eGHHzY33XSTU9mDDz5ogoODs9wm3d13323at2/vVPboo4+aunXrGmOMWbJkiQkODnYarbpcZiNK9957r2PZbreb0NBQM3v2bMdrqV27trHb7Y46qampxt/f33z11VeZ7iM5Odl07tzZSDLR0dGmd+/e5u233zYpKSmOOpmNKEVHR5u0tDRHWe3atc0NN9zgWL506ZIJCAgwCxYsMMbkPKKUmS5dupjRo0c7HY8mTZpkqKfLRpQyG9E0xphp06aZq6++2rG8ZMkSExgYmGE0tUmTJmbChAlZxgQAnoIRJQAoJMaYQm3/2muvVWBgoMqXL6+ffvpJixYtUlhYmCRp2LBhOn78uD744AO1atVKixcvVr169bRq1SpJ0o4dO9SkSRNVqFAh07Z/++03XXfddU5l1113nX777TensmbNmjkt//TTT5o0aZICAwMdP4MHD9axY8d07ty5TPd1/vx5+fn5OZUZY2Sz2XI8BlnFuXfvXqWlpal9+/aKjo5WtWrV1LdvX33wwQdZxpGuYcOGjt9tNpvCw8MVHx/veH379u1TUFCQ4/VVqFBBKSkp2r9/f6btBQQE6PPPP9e+ffv09NNPKzAwUKNHj1aLFi2yjaVevXry8vq//6bDwsLUoEEDx7K3t7cqVqzoiC0naWlpmjx5sho0aKAKFSooMDBQX331lQ4fPuxUr2nTprlqzyo2Nlb79u3Tjz/+KEmKi4tTr169Moym+vv753gOAMATuO6JTwCAk5o1a8pmsxXaTHOLFi1S3bp1VbFixUxv4wsKClK3bt3UrVs3TZkyRR06dNCUKVPUvn17x616BWX9EJycnKyJEyeqZ8+eGepak6F0lSpV0s6dO53KatWqpcTERB07dkwRERH5ji8oKEjbtm3T2rVrtXLlSo0bN04TJkzQ5s2bs7z1sXTp0k7LNpvNcUticnKymjZtmunMbSEhIdnGUr16dVWvXl2DBg3SU089pVq1amnRokUaMGBAruPILracvPjii3r11Vc1Y8YMNWjQQAEBARo5cmSGWwYzu000N0JDQ9WtWzfNmzdPMTEx+vLLL7V27doM9U6fPp3jsQIAT8CIEgAUkgoVKqhDhw6aNWuWzp49m2F9QWegi4qKUvXq1XP1rJPNZlOdOnUccTRs2FA7duzQ6dOnM61/9dVXa/369U5l69evV926dbPdzzXXXKPdu3erRo0aGX4uHx25XJMmTfT77787jcDdcccd8vHx0QsvvJDpNunHLqs4a9WqJW9vb0lSqVKl1K5dO73wwgv6+eefdfDgQX3zzTfZvo7sXt/evXsVGhqa4fWVLVs21+1UrVpVZcqUybRfFJb169ere/fuuvfee9WoUSNVq1ZNe/bsyXM7Pj4+kpTp82+DBg3SokWL9Oabb6p69eoZRvvSR96aNGmSvxcBAEWIESUAKESzZs3SddddpxYtWmjSpElq2LChLl26pFWrVmn27NlOt7IdOXIkw3fTREdH53mfO3bs0Pjx49W3b1/VrVtXPj4++vbbb/XOO+/o8ccflyTdddddeu6559SjRw9NnTpVERER2r59uyIjI9WqVSs9+uij6tWrl5o0aaJ27drp008/1dKlS/X1119nu+9x48apa9euqlKliu644w55eXnpp59+0i+//KIpU6Zkuk3btm2VnJysXbt2Ob4DKCoqSq+88ooeeughJSUlqV+/fqpatar++usvvffeewoMDNT06dM1evRoNW/eXJMnT1bv3r21YcMGvf7663rjjTckSZ999pn++OMP3XjjjSpfvry++OIL2e121a5dO8/HVZLuuecevfjii+revbsmTZqkypUr69ChQ1q6dKkee+wxVa5cOcM2EyZM0Llz59S5c2dFR0crISFBM2fOdEzjXlRq1qypjz76SD/88IPKly+vl19+WSdOnMgx+bUKDQ2Vv7+/VqxYocqVK8vPz8+RJHbo0EHBwcGaMmWKJk2alGHbH3/8Ub6+vmrVqpVLXhMAFCZGlACgEFWrVk3btm1T27ZtNXr0aNWvX1/t27fX6tWrNXv2bKe6L730kpo0aeL08/nnn+d5n5UrV1bVqlU1ceJEtWzZUtdcc41effVVTZw4UU899ZSkf0cFVq5cqdDQUHXu3FkNGjTQ888/7xiF6dGjh1599VW99NJLqlevnubOnat58+apTZs22e67Q4cO+uyzz7Ry5Uo1b95c//nPf/TKK69km/BVrFhRt912W4bb2YYOHaqVK1fqyJEjuu2221SnTh0NGjRIwcHBGjNmjKR/R3g+/PBDLVy4UPXr19e4ceM0adIkxcbGSvp3drylS5fqpptu0tVXX605c+ZowYIFqlevXp6PqySVKVNG69atU5UqVdSzZ09dffXVGjhwoFJSUhQcHJzpNq1bt9Yff/yhfv36qU6dOurUqZOOHz+ulStX5jthy4+nn35a11xzjTp06KA2bdooPDxcPXr0yHM7pUqV0syZMzV37lxFRkaqe/fujnVeXl6KjY1VWlqa+vXrl2HbBQsW6J577lGZMmUK8lIAoEjYTGE/bQwAQA5+/vlntW/fXvv371dgYKC7w0EBDBw4UCdPntTy5cudyk+dOqXatWtry5YtiomJcVN0AJB73HoHAHC7hg0batq0aTpw4IDTzG4oPhITE7Vz507Nnz8/Q5IkSQcPHtQbb7xBkgSg2GBECQAAFFibNm20adMmDRkyRK+88oq7wwGAAiNRAgAAAAALJnMAAAAAAAsSJQAAAACwIFECAAAAAAsSJQAAAACwIFECAAAAAAsSJQAAAACwIFECAAAAAAsSJQAAAACw+H9x/bI3fcfbzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "print(\"Computing CLIPScore distributions for EEG → Caption...\")\n",
    "\n",
    "eeg_emb = eeg_emb_test        # [N_test, d]\n",
    "txt_bank = text_emb_bank      # [N_caps, d]\n",
    "true_cap_idx = caption_idx_test  # [N_test]\n",
    "\n",
    "N = eeg_emb.size(0)\n",
    "\n",
    "matched_scores = []\n",
    "mismatched_scores = []\n",
    "\n",
    "# --- Matched EEG↔Caption similarity ---\n",
    "for i in range(N):\n",
    "    eeg_vec = eeg_emb[i : i+1]                           # [1, d]\n",
    "    cap_idx = true_cap_idx[i]\n",
    "    txt_vec = txt_bank[cap_idx : cap_idx+1]             # [1, d]\n",
    "    sim = torch.matmul(eeg_vec, txt_vec.T).item()\n",
    "    matched_scores.append(sim)\n",
    "\n",
    "# --- Mismatched EEG↔Random Caption similarity ---\n",
    "rng = np.random.default_rng()\n",
    "for i in range(N):\n",
    "    eeg_vec = eeg_emb[i : i+1]\n",
    "    \n",
    "    # Select random caption not equal to true one\n",
    "    cap_idx = true_cap_idx[i]\n",
    "    choices = [j for j in range(len(txt_bank)) if j != cap_idx]\n",
    "    neg_idx = rng.choice(choices)\n",
    "    \n",
    "    txt_vec = txt_bank[neg_idx : neg_idx+1]\n",
    "    sim = torch.matmul(eeg_vec, txt_vec.T).item()\n",
    "    mismatched_scores.append(sim)\n",
    "\n",
    "matched_scores  = np.array(matched_scores)\n",
    "mismatched_scores = np.array(mismatched_scores)\n",
    "\n",
    "print(f\"Matched CLIPScore: mean={matched_scores.mean():.4f}, std={matched_scores.std():.4f}\")\n",
    "print(f\"Mismatched CLIPScore: mean={mismatched_scores.mean():.4f}, std={mismatched_scores.std():.4f}\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(matched_scores, bins=40, alpha=0.7, label='Matched pairs', density=True)\n",
    "plt.hist(mismatched_scores, bins=40, alpha=0.7, label='Mismatched pairs', density=True)\n",
    "plt.xlabel(\"CLIPScore (Cosine Similarity)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"EEG → Caption CLIPScore: Matched vs. Mismatched\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Example #0\n",
      "**Ground truth caption:** Wooden diningtable with matching chairs on hardwood floor\n",
      "\n",
      "\u001b[91mTop-1: Sheep resting on rocky stone steps  (Sim=0.0824)\u001b[0m\n",
      "\u001b[91mTop-2: White flower bloom with green surroundings  (Sim=0.0811)\u001b[0m\n",
      "\u001b[91mTop-3: Dog wrapped in a white towel indoors  (Sim=0.0782)\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "Example #1040\n",
      "**Ground truth caption:** Blue car parked on asphalt near mountains\n",
      "\n",
      "\u001b[91mTop-1: Person smiling with child by a fence  (Sim=0.0904)\u001b[0m\n",
      "\u001b[91mTop-2: Gray cat sitting by green bushes  (Sim=0.0899)\u001b[0m\n",
      "\u001b[91mTop-3: Person folding clothes outside on grass  (Sim=0.0859)\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "Example #2080\n",
      "**Ground truth caption:** Blue sofa with two black cushions nearby table\n",
      "\n",
      "\u001b[91mTop-1: Excited dog with tongue out on carpet  (Sim=0.1011)\u001b[0m\n",
      "\u001b[91mTop-2: Person wearing glasses sitting beside a lamp  (Sim=0.0956)\u001b[0m\n",
      "\u001b[91mTop-3: Dog standing on a mossy stone wall  (Sim=0.0950)\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "Example #3120\n",
      "**Ground truth caption:** Cow grazing on a grassy open field\n",
      "\n",
      "\u001b[91mTop-1: Person holding a handmade stuffed animal indoors  (Sim=0.0998)\u001b[0m\n",
      "\u001b[91mTop-2: tvmonitor on a desk with a plush banana  (Sim=0.0994)\u001b[0m\n",
      "\u001b[91mTop-3: Person sitting on blue rocking toy outdoors  (Sim=0.0983)\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "Example #4160\n",
      "**Ground truth caption:** White sofa with pillows beside orange wall\n",
      "\n",
      "\u001b[91mTop-1: Gray cat perched on a wooden fence  (Sim=0.1168)\u001b[0m\n",
      "\u001b[91mTop-2: Bird standing on a wooden railing by plants  (Sim=0.1074)\u001b[0m\n",
      "\u001b[91mTop-3: Bird standing on a seaside railing  (Sim=0.1004)\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## EEG → Caption Retrieval: Visualization Examples (Optional)\n",
    "\n",
    "# %%\n",
    "def visualize_eeg_caption_retrieval(\n",
    "    eeg_emb,\n",
    "    text_emb_bank,\n",
    "    meta_df,\n",
    "    unique_captions,\n",
    "    k=3,\n",
    "    num_examples=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize EEG→Caption retrieval examples.\n",
    "    Shows:\n",
    "        - Ground truth caption\n",
    "        - Top-k retrieved captions\n",
    "        - CLIPScore\n",
    "    \"\"\"\n",
    "    N = len(eeg_emb)\n",
    "    step = max(1, N // num_examples)\n",
    "\n",
    "    for i in range(0, N, step):\n",
    "        if num_examples <= 0:\n",
    "            break\n",
    "        num_examples -= 1\n",
    "\n",
    "        eeg_vec = eeg_emb[i : i+1]    # [1, d]\n",
    "        sims = (eeg_vec @ text_emb_bank.T).squeeze(0)  # [N_caps]\n",
    "        top_scores, top_idx = torch.topk(sims, k=k)\n",
    "\n",
    "        gt_caption = meta_df.iloc[i][\"abstracted\"]\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Example #{i}\")\n",
    "        print(f\"**Ground truth caption:** {gt_caption}\\n\")\n",
    "\n",
    "        for rank in range(k):\n",
    "            cap = unique_captions[top_idx[rank]]\n",
    "            score = top_scores[rank].item()\n",
    "            is_correct = (cap == gt_caption)\n",
    "\n",
    "            color = \"\\033[92m\" if is_correct else \"\\033[91m\"\n",
    "            reset = \"\\033[0m\"\n",
    "\n",
    "            print(f\"{color}Top-{rank+1}: {cap}  (Sim={score:.4f}){reset}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# Run visualization\n",
    "visualize_eeg_caption_retrieval(\n",
    "    eeg_emb_test,\n",
    "    text_emb_bank,\n",
    "    all_meta.iloc[len(eeg_train_meta_full) + len(eeg_val_meta_full):],  # test rows only\n",
    "    unique_captions,\n",
    "    k=3,\n",
    "    num_examples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG Env",
   "language": "python",
   "name": "eeg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
